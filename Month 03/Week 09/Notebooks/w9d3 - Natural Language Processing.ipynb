{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b411dbe2",
   "metadata": {},
   "source": [
    "# Summer of Code - Artificial Intelligence\n",
    "\n",
    "## Week 09: Deep Learning\n",
    "\n",
    "### Day 03: Natural Language Processing\n",
    "\n",
    "In this notebook, we will learn the theoretical foundations of **Natural Language Processing (NLP)** using NLTK and spaCy libraries in Python.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c0bfb82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'nltk' from 'C:\\\\Users\\\\DIPLAB\\\\AppData\\\\Roaming\\\\Python\\\\Python312\\\\site-packages\\\\nltk\\\\__init__.py'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "53384eec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\DIPLAB\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\DIPLAB\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\DIPLAB\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\DIPLAB\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\DIPLAB\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\DIPLAB\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger_eng.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt_tab') # Download the Punkt tokenizer table\n",
    "nltk.download('punkt') # Download the Punkt tokenizer models\n",
    "nltk.download('stopwords') # Download the stopwords corpus\n",
    "nltk.download('wordnet') # Download the WordNet corpus\n",
    "nltk.download('averaged_perceptron_tagger') # Download the POS tagger\n",
    "nltk.download('averaged_perceptron_tagger_eng') # Download the English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b72e619",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The quick brown foxes, jumping over the lazy dog, were amazed by the dog's indifference! (Video: https://bit.ly/3xYZ)\n"
     ]
    }
   ],
   "source": [
    "text = \"The quick brown foxes, jumping over the lazy dog, were amazed by the dog's indifference! (Video: https://bit.ly/3xYZ)\"\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f530da97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"the quick brown foxes, jumping over the lazy dog, were amazed by the dog's indifference! (video: https://bit.ly/3xyz)\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "32cb2f19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'quick',\n",
       " 'brown',\n",
       " 'foxes',\n",
       " ',',\n",
       " 'jumping',\n",
       " 'over',\n",
       " 'the',\n",
       " 'lazy',\n",
       " 'dog',\n",
       " ',',\n",
       " 'were',\n",
       " 'amazed',\n",
       " 'by',\n",
       " 'the',\n",
       " 'dog',\n",
       " \"'s\",\n",
       " 'indifference',\n",
       " '!',\n",
       " '(',\n",
       " 'Video',\n",
       " ':',\n",
       " 'https',\n",
       " ':',\n",
       " '//bit.ly/3xYZ',\n",
       " ')']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c5e5361f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'t', 'over', 'have', \"didn't\", 'before', 'had', \"don't\", 'into', \"she'll\", \"they're\", 'now', 'while', 'so', 'are', 'any', 'during', 'these', 'haven', 'she', \"he'll\", 'won', 'an', 'couldn', 'myself', \"it'll\", 'was', 'my', 'no', 'out', \"you're\", \"should've\", 'yourselves', 'didn', 'this', 'will', 'ourselves', 'each', 'under', 'doing', 'i', 'some', 'he', 'our', 're', \"needn't\", 'only', 'm', 'hers', 'isn', 'ours', \"wasn't\", 'own', 'yourself', 'further', 'just', 'wouldn', 'were', 'shan', 'is', 'wasn', \"we're\", 'from', \"weren't\", 'here', \"you've\", 'up', 'to', 'you', \"they'd\", 'having', 'we', 'all', 'few', \"he's\", 'his', \"shouldn't\", 'if', 'weren', 'on', 'its', \"doesn't\", 'or', 'theirs', 'again', 'such', 'but', 'why', 'me', 'by', 'do', 'down', 'through', \"we've\", 'for', 'above', \"aren't\", 'who', 'about', \"i'm\", 'how', 'the', \"you'd\", \"they'll\", 'ma', 'than', 've', \"she'd\", 'they', 'yours', \"isn't\", \"couldn't\", 'hadn', 's', 'your', 'both', 'not', 'a', 'did', 'same', 'has', 'too', \"he'd\", 'been', \"it'd\", 'don', 'aren', 'below', \"haven't\", \"i'll\", \"it's\", 'between', 'himself', 'd', \"won't\", 'and', 'mightn', \"i'd\", 'll', 'after', 'once', 'those', 'very', 'itself', \"we'll\", \"wouldn't\", 'because', 'o', \"mightn't\", 'what', 'him', 'off', 'themselves', 'does', \"mustn't\", 'be', 'until', 'doesn', 'nor', 'can', 'then', 'being', 'that', \"they've\", 'when', 'their', 'ain', 'her', \"we'd\", 'more', 'there', \"i've\", \"you'll\", \"shan't\", 'y', 'should', 'most', 'as', \"that'll\", \"hadn't\", 'hasn', 'where', 'am', 'whom', 'them', 'needn', 'at', 'of', \"hasn't\", 'against', 'mustn', 'it', \"she's\", 'which', 'other', 'in', 'with', 'shouldn', 'herself'}\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c260f561",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['quick', 'brown', 'foxes', ',', 'jumping', 'lazy', 'dog', ',', 'amazed', 'dog', \"'s\", 'indifference', '!', '(', 'Video', ':', 'https', ':', '//bit.ly/3xYZ', ')']\n"
     ]
    }
   ],
   "source": [
    "filtered_words = [word for word in word_tokenize(text) if word.lower() not in stop_words]\n",
    "print(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "67f7ac5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['quick', 'brown', 'foxes', 'jumping', 'lazy', 'dog', 'amazed', 'dog', 'indifference', 'Video', 'https']\n"
     ]
    }
   ],
   "source": [
    "# Remove non alphanumeric characters\n",
    "import re\n",
    "\n",
    "# give a list of tokens, keep only those tokens that are alphanumeric\n",
    "alphanumeric_words = [word for word in filtered_words if re.match('^[a-zA-Z0-9]+$', word)]\n",
    "print(alphanumeric_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cc045801",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('quick', 'JJ'), ('brown', 'NN'), ('foxes', 'NNS'), ('jumping', 'VBG'), ('lazy', 'JJ'), ('dog', 'NN'), ('amazed', 'VBD'), ('dog', 'JJ'), ('indifference', 'NN'), ('Video', 'NNP'), ('https', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "# perform a pos tagging on the alphanumeric words\n",
    "pos_tags = nltk.pos_tag(alphanumeric_words)\n",
    "print(pos_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "248d68fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     --- ------------------------------------ 1.0/12.8 MB 12.7 MB/s eta 0:00:01\n",
      "     ---------------------- ----------------- 7.1/12.8 MB 24.3 MB/s eta 0:00:01\n",
      "     ----------------------------------- --- 11.5/12.8 MB 23.3 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 12.8/12.8 MB 22.3 MB/s  0:00:00\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.8.0\n",
      "\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "511f4528",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The quick brown foxes, jumping over the lazy dog, were amazed by the dog's indifference! (Video: https://bit.ly/3xYZ)\n"
     ]
    }
   ],
   "source": [
    "text = \"The quick brown foxes, jumping over the lazy dog, were amazed by the dog's indifference! (Video: https://bit.ly/3xYZ)\"\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "039e3496",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the quick brown foxes, jumping over the lazy dog, were amazed by the dog's indifference! (video: https://bit.ly/3xyz)\n"
     ]
    }
   ],
   "source": [
    "text = text.lower()\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e270f76b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'spacy' from 'C:\\\\Users\\\\DIPLAB\\\\AppData\\\\Roaming\\\\Python\\\\Python312\\\\site-packages\\\\spacy\\\\__init__.py'>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d580d562",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.doc.Doc"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(text)\n",
    "type(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d5192ec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[the, quick, brown, foxes, ,, jumping, over, the, lazy, dog, ,, were, amazed, by, the, dog, 's, indifference, !, (, video, :, https://bit.ly/3xyz, )]\n"
     ]
    }
   ],
   "source": [
    "tokens = [token for token in doc]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "400e0bd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[the,\n",
       " quick,\n",
       " brown,\n",
       " foxes,\n",
       " jumping,\n",
       " over,\n",
       " the,\n",
       " lazy,\n",
       " dog,\n",
       " were,\n",
       " amazed,\n",
       " by,\n",
       " the,\n",
       " dog,\n",
       " 's,\n",
       " indifference,\n",
       " video,\n",
       " https://bit.ly/3xyz]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_tokens = [token for token in tokens if not token.is_punct and not token.is_space]\n",
    "clean_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f8dcdf18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[quick,\n",
       " brown,\n",
       " foxes,\n",
       " jumping,\n",
       " lazy,\n",
       " dog,\n",
       " amazed,\n",
       " dog,\n",
       " indifference,\n",
       " video,\n",
       " https://bit.ly/3xyz]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_tokens = [token for token in tokens if not token.is_stop and not token.is_punct]\n",
    "clean_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f48d9cb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quick: ADJ\n",
      "brown: ADJ\n",
      "foxes: NOUN\n",
      "jumping: VERB\n",
      "lazy: ADJ\n",
      "dog: NOUN\n",
      "amazed: ADJ\n",
      "dog: NOUN\n",
      "indifference: NOUN\n",
      "video: NOUN\n",
      "https://bit.ly/3xyz: PROPN\n"
     ]
    }
   ],
   "source": [
    "# pos tagging using spacy\n",
    "for token in clean_tokens:\n",
    "    print(f\"{token.text}: {token.pos_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c3913f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
