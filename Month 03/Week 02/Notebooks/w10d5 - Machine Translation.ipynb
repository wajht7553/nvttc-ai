{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b40606a1",
   "metadata": {},
   "source": [
    "# Summer of Code - Artificial Intelligence\n",
    "\n",
    "## Week 10: Deep Learning\n",
    "\n",
    "### Day 05: Machine Translation\n",
    "\n",
    "In this notebook, we will explore **Machine Translation** using **Bidirectional RNNs** in PyTorch.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b27b1ac",
   "metadata": {},
   "source": [
    "# Neural Machine Translation\n",
    "\n",
    "Neural Machine Translation (NMT) is another NLP task where we translate text from one language to another using neural networks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a53b138",
   "metadata": {},
   "source": [
    "## Encoder-Decoder Architecture\n",
    "\n",
    "The most common architecture for NMT is the Encoder-Decoder architecture. The encoder processes the input sentence and encodes it into a fixed-length context vector, which is then used by the decoder to generate the translated sentence.\n",
    "\n",
    "<img src=\"images/enc_dec.png\" alt=\"Encoder-Decoder Architecture\" width=\"600\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d72a1e2",
   "metadata": {},
   "source": [
    "### English to Urdu Translation Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f453e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from zipfile import ZipFile\n",
    "from urllib.request import urlopen, Request\n",
    "\n",
    "\n",
    "url = \"https://www.manythings.org/anki/urd-eng.zip\"\n",
    "req = Request(\n",
    "    url,\n",
    "    headers={\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "    },\n",
    ")\n",
    "with urlopen(req) as response:\n",
    "    with open(\"urd-eng.zip\", \"wb\") as f:\n",
    "        f.write(response.read())\n",
    "with ZipFile(\"urd-eng.zip\", \"r\") as zip_ref:\n",
    "    zip_ref.extractall(\"./eng-urd\")\n",
    "print(\"Dataset downloaded and extracted.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d946e70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hi.\\tسلام۔\\tCC-BY 2.0 (France) Attribution: tatoeba.org #538123 (CM) & #9020897 (nusrat)\\n',\n",
       " 'Help!\\tمدد۔\\tCC-BY 2.0 (France) Attribution: tatoeba.org #435084 (lukaszpp) & #1462368 (nabeel_tahir)\\n',\n",
       " 'Thanks.\\tشکریہ۔\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2057650 (nava) & #9020893 (nusrat)\\n',\n",
       " 'We won.\\tہم جیت گئے۔\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2107675 (CK) & #2123755 (nabeel_tahir)\\n',\n",
       " 'Beat it.\\tبھاگ جائو۔\\tCC-BY 2.0 (France) Attribution: tatoeba.org #37902 (CM) & #1610833 (nabeel_tahir)\\n']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"./eng-urd/urd.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = f.readlines()\n",
    "\n",
    "data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "218a8237",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Hi.', 'سلام۔'], ['Help!', 'مدد۔'], ['Thanks.', 'شکریہ۔']]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_target = [line.strip().split(\"\\t\")[:2] for line in data]\n",
    "source_target[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "84edf46e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Hey, look at this.', 'یہ دیکھو۔'],\n",
       " ['I keep a dog.', 'میں کتا رکھتا ہوں۔'],\n",
       " [\"I just don't know what to say.\", 'مجھے ابھی نہیں پتا کیا کہنا ہے ۔']]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "np.random.shuffle(source_target)\n",
    "source_target[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "96d82fb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hey, look at this. => یہ دیکھو۔\n",
      "I keep a dog. => میں کتا رکھتا ہوں۔\n",
      "I just don't know what to say. => مجھے ابھی نہیں پتا کیا کہنا ہے ۔\n"
     ]
    }
   ],
   "source": [
    "source_sentences, target_sentences = zip(*source_target)\n",
    "for i in range(3):\n",
    "    print(f\"{source_sentences[i]} => {target_sentences[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82a23a0",
   "metadata": {},
   "source": [
    "### Build Vocabulary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9ba2a003",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sentences):\n",
    "    return [s.lower().split() for s in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e8ea2707",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([['hey,', 'look', 'at', 'this.'],\n",
       "  ['i', 'keep', 'a', 'dog.'],\n",
       "  ['i', 'just', \"don't\", 'know', 'what', 'to', 'say.']],\n",
       " [['یہ', 'دیکھو۔'],\n",
       "  ['میں', 'کتا', 'رکھتا', 'ہوں۔'],\n",
       "  ['مجھے', 'ابھی', 'نہیں', 'پتا', 'کیا', 'کہنا', 'ہے', '۔']])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_tokens = tokenize(source_sentences)\n",
    "target_tokens = tokenize(target_sentences)\n",
    "\n",
    "source_tokens[:3], target_tokens[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bd8b2444",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Vocabulary Size: 1500\n",
      "Urdu Vocabulary Size: 1500\n",
      "Most common English words: [('i', 284), ('the', 265), ('to', 223), ('you', 195), ('a', 167)]\n",
      "Most common Urdu words: [('میں', 376), ('ہے۔', 324), ('نے', 182), ('اس', 155), ('وہ', 149)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "\n",
    "def build_vocabulary(tokenized_sentences, source=True, max_vocab_size=1000):\n",
    "    word_counts = Counter()\n",
    "    for s in tokenized_sentences:\n",
    "        word_counts.update(s)\n",
    "\n",
    "    # Create vocabulary with special tokens\n",
    "    if source:\n",
    "        vocab = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
    "        most_common = word_counts.most_common(max_vocab_size - 2)\n",
    "    else:\n",
    "        vocab = {\"<PAD>\": 0, \"<UNK>\": 1, \"<SOS>\": 2, \"<EOS>\": 3}\n",
    "        most_common = word_counts.most_common(max_vocab_size - 4)\n",
    "    # Add most common words\n",
    "    for word, _ in most_common:\n",
    "        vocab[word] = len(vocab)\n",
    "\n",
    "    return vocab, word_counts\n",
    "\n",
    "\n",
    "source_vocab, source_word_counts = build_vocabulary(source_tokens, max_vocab_size=1500)\n",
    "target_vocab, target_word_counts = build_vocabulary(\n",
    "    target_tokens, source=False, max_vocab_size=1500\n",
    ")\n",
    "print(\"English Vocabulary Size:\", len(source_vocab))\n",
    "print(\"Urdu Vocabulary Size:\", len(target_vocab))\n",
    "print(\"Most common English words:\", source_word_counts.most_common(5))\n",
    "print(\"Most common Urdu words:\", target_word_counts.most_common(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cbe152e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<PAD>': 0, '<UNK>': 1, '<SOS>': 2, '<EOS>': 3, 'میں': 4, 'ہے۔': 5, 'نے': 6, 'اس': 7, 'وہ': 8, 'کے': 9, 'کو': 10, 'نہیں': 11, 'کی': 12, 'سے': 13, 'ٹام': 14, 'مجھے': 15, 'تم': 16, 'کیا': 17, 'کہ': 18, 'ہوں۔': 19, 'کا': 20, 'ہو': 21, 'ہے': 22, 'یہ': 23, 'آپ': 24, 'ایک': 25, 'کر': 26, 'ہے؟': 27, 'تھا۔': 28, 'میرے': 29, 'اپنی': 30, 'ہیں۔': 31, 'بہت': 32, 'اور': 33, 'رہا': 34, 'گا۔': 35, 'کافی': 36, 'پہ': 37, 'میری': 38, 'کچھ': 39, 'اسے': 40, 'بھی': 41, 'ہم': 42, 'گھر': 43, 'ہو؟': 44, 'گیا': 45, 'مریم': 46, 'تمہیں': 47, 'نہ': 48, 'ہوا': 49, 'تھی۔': 50, 'زیادہ': 51, 'ہو۔': 52, 'کرنا': 53, 'رہی': 54, 'رہے': 55, 'ہی': 56, 'اپنے': 57, 'کرنے': 58, 'سکتے': 59, 'میرا': 60, 'سال': 61, 'وقت': 62, 'گیا۔': 63, 'گی۔': 64, 'پسند': 65, 'پاس': 66, 'تک': 67, 'تھا': 68, 'کوئی': 69, 'جا': 70, 'لئیے': 71, 'ابھی': 72, 'آج': 73, 'کبھی': 74, 'تو': 75, 'پر': 76, 'یہاں': 77, 'ساتھ': 78, '۔': 79, 'کسی': 80, 'جلدی': 81, 'تمھیں': 82, 'آ': 83, 'سب': 84, 'گاڑی': 85, 'گئے': 86, 'کرتا': 87, 'جائو': 88, 'کام': 89, 'جب': 90, 'ٹھیک': 91, 'گے۔': 92, 'ان': 93, 'لگ': 94, 'سکتا': 95, 'بعد': 96, 'خیال': 97, 'اب': 98, 'کیا۔': 99, 'پتہ': 100, 'کل': 101, 'کوشش': 102, 'انہوں': 103, 'ہوں': 104, 'کتاب': 105, 'رات': 106, 'ہر': 107, 'چاہتا': 108, 'واپس': 109, 'تھے۔': 110, 'پہلی': 111, 'کی۔': 112, 'بڑی': 113, 'ہمیں': 114, 'ضرورت': 115, 'پہلے': 116, 'دے': 117, 'کرو۔': 118, 'اپنا': 119, 'طرح': 120, 'برائے': 121, 'جو': 122, 'انگریزی': 123, 'پیسے': 124, 'دفعہ': 125, 'دیا۔': 126, 'ہوئے': 127, 'بارش': 128, 'گئی': 129, 'مدد': 130, 'بھائی': 131, 'کس': 132, 'ابو': 133, 'ڈاکٹر': 134, 'زندگی': 135, 'لیا۔': 136, 'دو': 137, 'کیسے': 138, 'باہر': 139, 'دس': 140, 'لے': 141, 'آئے': 142, 'تین': 143, 'بس': 144, 'دن': 145, 'تھوڑا': 146, 'دیر': 147, 'ہفتے': 148, 'کھانے': 149, 'بہن': 150, 'کم': 151, 'جانتا': 152, 'جواب': 153, 'گھڑی': 154, 'مگر': 155, 'غلط': 156, 'ہمیشہ': 157, 'صبح': 158, 'نہیں۔': 159, 'بجے': 160, 'مہربانی': 161, 'انتظار': 162, 'لوگ': 163, 'بند': 164, 'جانا': 165, 'بتایا': 166, 'پانچ': 167, 'گے؟': 168, 'شروع': 169, 'بڑا': 170, 'دی': 171, 'لئے': 172, 'کرے': 173, 'ہاتھ': 174, 'لغت': 175, 'ٹی': 176, '؟': 177, 'زبان': 178, 'خوبصورت': 179, 'اتنی': 180, 'جائے': 181, 'اگر': 182, 'بچے': 183, 'وہاں': 184, 'ادھار': 185, 'صرف': 186, 'جانے': 187, 'تھے': 188, 'جیت': 189, 'چاہئیے۔': 190, 'موسم': 191, 'کرتے': 192, 'بیمار': 193, 'ہے،': 194, 'بات': 195, 'نظر': 196, 'کتابیں': 197, 'سارے': 198, 'لگا': 199, 'دیا': 200, 'تیراکی': 201, 'بتا': 202, 'فرانسیسی': 203, 'بال': 204, 'تمھارے': 205, 'پچھلے': 206, 'پتا': 207, 'پہنچ': 208, 'طرف': 209, 'فون': 210, 'دور': 211, 'مصروف': 212, 'ختم': 213, 'ہونے': 214, 'کتنی': 215, 'دوست': 216, 'جاپان': 217, 'ہوٹل': 218, 'کھانا': 219, 'لیا': 220, 'پوری': 221, 'نیا': 222, 'کہانی': 223, 'بارے': 224, 'گے': 225, 'مشکل': 226, 'کرو': 227, 'تیز': 228, 'لگتا': 229, 'عمر': 230, 'اسی': 231, 'دو۔': 232, 'تھکا': 233, 'اچھا': 234, 'امیر': 235, 'سکتا۔': 236, 'خود': 237, 'امید': 238, 'سونے': 239, 'چلا': 240, 'والد': 241, 'والے': 242, 'مجھ': 243, 'کیا؟': 244, 'مادری': 245, 'ہوتا': 246, 'ہفتہ': 247, 'رہ': 248, 'کمرے': 249, 'دیکھا۔': 250, 'دیکھا': 251, 'آئے۔': 252, 'سکتی': 253, 'غلطی': 254, 'کرنا۔': 255, 'کہاں': 256, 'کاش': 257, 'ماں': 258, 'چھوڑ': 259, 'لوگوں': 260, 'گرم': 261, 'لو۔': 262, 'کروں؟': 263, 'رہتا': 264, 'والدہ': 265, 'عورت': 266, 'یاد': 267, 'سکتے۔': 268, 'اچھے': 269, 'خاندان': 270, 'پڑھنے': 271, 'ہیں': 272, 'کب': 273, 'پورے': 274, 'فرق': 275, 'شاید': 276, 'آیا۔': 277, 'صحیح': 278, 'اتنے': 279, 'نئی': 280, 'آیا': 281, 'دوپہر': 282, 'ریل': 283, 'جاتی': 284, 'زندہ': 285, 'لڑکی': 286, 'رہا۔': 287, 'کال': 288, 'دی۔': 289, 'دوبارہ': 290, 'تمہاری': 291, 'یا': 292, 'چیز': 293, 'ہوئی': 294, 'تمہارے': 295, 'چاہیے؟': 296, 'استعمال': 297, 'دیکھو۔': 298, 'کہنا': 299, 'چل': 300, 'فیصلہ': 301, 'ہماری': 302, 'پیار': 303, 'ممکن': 304, 'بیٹی': 305, 'کرے۔': 306, 'ملک': 307, 'جائے۔': 308, 'گر': 309, 'گئی۔': 310, 'اگلے': 311, 'میکسیکو': 312, 'گم': 313, 'لیٹ': 314, 'تمہارا': 315, 'سکول': 316, 'آتے': 317, 'جتنا': 318, 'آرام': 319, 'ہوتی۔': 320, 'چائے': 321, 'اوپر': 322, 'مر': 323, 'ڈالر': 324, 'تھی': 325, 'کھڑکی': 326, 'پانی': 327, 'کھا': 328, 'چاہو': 329, 'آگے': 330, 'گا؟': 331, 'بستر': 332, 'پڑے': 333, 'مت': 334, 'کھیل': 335, 'فوت': 336, 'گا': 337, 'حاصل': 338, 'اندر': 339, 'زور': 340, 'پھر': 341, 'رہتے': 342, 'سڑک': 343, 'ملے': 344, 'کیسا': 345, 'ہیں؟': 346, 'خط': 347, 'سالگرہ': 348, 'عجیب': 349, 'تفصیل': 350, 'تھا؟': 351, 'خوش': 352, 'لی': 353, 'ویل': 354, 'چیئر': 355, 'دیکھی': 356, 'مرمت': 357, 'رکھ': 358, 'لی۔': 359, 'کیوں': 360, 'پکانے': 361, 'بچوں': 362, 'کرنی': 363, 'چائیے۔': 364, 'رہو۔': 365, 'جس': 366, 'کیونکہ': 367, 'ہونا': 368, 'تب': 369, 'لیئے': 370, 'چاہتا۔': 371, 'کیسی': 372, '۱۰۰': 373, 'چکی': 374, 'گرین': 375, 'پی': 376, 'چاہتے': 377, 'بلا': 378, 'آئی': 379, 'نمبر': 380, 'کتنا': 381, 'نفرت': 382, 'دل': 383, 'پکڑ': 384, 'سولہ': 385, 'اکیلے': 386, 'زمین': 387, 'صفائی': 388, 'دنیا': 389, 'گھنٹے': 390, 'بھروسہ': 391, 'جاپانی': 392, 'الفاظوں': 393, 'آٹھ': 394, 'آتا۔': 395, 'والا': 396, 'جاتے': 397, 'چاہئیے': 398, 'اتنا': 399, 'کدھر': 400, 'چوری': 401, 'اچھی': 402, 'چھہ': 403, '۱۹۸۳': 404, 'بغیر': 405, 'سارا': 406, 'پھول': 407, 'کھلا': 408, 'انعام': 409, 'تر': 410, 'آنکھیں': 411, 'بتائی': 412, 'ڈھیڑ': 413, 'ساری': 414, 'تصویریں': 415, 'استاد': 416, 'سمجھنا': 417, 'تصویر': 418, 'چلو': 419, 'ہفتوں': 420, 'پڑا': 421, 'ایماندار': 422, 'دعوت': 423, 'مرضی': 424, 'ادا': 425, 'امتحان': 426, 'ہوگا۔': 427, 'لگی': 428, 'پیانو': 429, 'عادت': 430, 'جان': 431, 'بچا': 432, 'کتنے': 433, 'ہار': 434, 'نانا': 435, 'شوق': 436, 'فلم': 437, 'چار': 438, 'شام': 439, 'پھل': 440, 'ہونا۔': 441, 'اٹھا': 442, 'سو': 443, 'بلکل': 444, 'چہل': 445, 'قدمی': 446, 'لا': 447, 'ڈر': 448, 'دوں': 449, 'دادا': 450, 'پیچھے': 451, 'قسم': 452, 'باتیں': 453, 'سامنے': 454, 'حادثہ': 455, 'دوسروں': 456, 'ترجیح': 457, 'سیاہ': 458, 'آسان': 459, 'یورپ': 460, 'عرصہ': 461, 'لے۔': 462, 'عام': 463, 'ہمارے': 464, 'جھوٹ': 465, 'نکل': 466, 'جائو۔': 467, 'گنا': 468, 'کماتا': 469, 'جاتی۔': 470, 'بڑے': 471, 'راز': 472, 'رکھنا': 473, 'چھٹی': 474, 'رنگ': 475, 'پینٹ': 476, 'دیتا': 477, 'خریدنے': 478, 'ذائقہ': 479, 'پوچھ': 480, 'سکتی۔': 481, 'سا': 482, 'عموماً': 483, 'وی': 484, 'دھیان': 485, 'دونوں': 486, 'قریب': 487, 'معذرت': 488, 'اجنبی': 489, 'بیس': 490, 'وجہ': 491, 'پڑھتا': 492, 'فٹ': 493, 'روزانہ': 494, 'گئے۔': 495, 'عمارت': 496, 'امی': 497, 'پڑھنا': 498, 'بزرگ': 499, 'مہینے': 500, 'اونچا': 501, 'ہوں؟': 502, 'زخمی': 503, 'معافی': 504, 'روشنی': 505, 'دکھ': 506, 'حقیقت': 507, 'چاہتی': 508, 'لیے': 509, 'تیرنا': 510, 'فورا': 511, 'کون': 512, 'قیام': 513, 'ٹینس': 514, 'پورا': 515, 'ٹرین': 516, 'دریا': 517, 'تھے؟': 518, 'ٹھنڈ': 519, 'اُس': 520, 'قصور': 521, 'چکا': 522, 'دیکھ': 523, 'آتی': 524, 'تمباکونوشی': 525, 'تھوڑی': 526, 'امریکا': 527, 'اگلی': 528, 'کتا': 529, 'رکھتا': 530, 'مل': 531, 'انہیوں': 532, 'جمع': 533, 'پرانی': 534, 'سٹوڈنٹ': 535, 'پرانے': 536, 'نام': 537, 'مریم۔': 538, 'مہربانی،': 539, 'آجاؤنگا': 540, 'محنت': 541, 'ٹیلی': 542, 'کالیں': 543, 'آتی۔': 544, 'منٹ': 545, 'اڑ': 546, 'مہنگا': 547, 'خرید': 548, 'خطرے': 549, 'مبتلا': 550, 'آئڈیا': 551, 'کیمرہ': 552, '۹۰': 553, 'والی': 554, 'ملا': 555, 'جانتے': 556, 'اتفاق': 557, 'دے۔': 558, 'کیک': 559, 'پیس': 560, 'کیئے': 561, 'چاچا': 562, 'ہنسے': 563, 'سامان': 564, 'پارٹی': 565, 'خریدے؟': 566, 'پہنچی۔': 567, 'امتہان': 568, 'آگ': 569, 'حیرت': 570, 'کالا': 571, 'بیگ': 572, 'لپیٹ': 573, 'سائز': 574, 'انڈے': 575, 'خوشبو': 576, 'فرانس': 577, 'ہوئی۔': 578, 'گرمی': 579, 'کھول': 580, 'دکھائیں۔': 581, 'جلد': 582, 'جاتا': 583, 'جدھر': 584, 'بتائیں': 585, 'نظریہ': 586, 'ڈھائی': 587, 'جائوں': 588, 'جیسے': 589, 'تلاش': 590, 'کا؟': 591, 'شکریہ': 592, 'تایا': 593, 'دوسرے': 594, 'مزیدار': 595, 'غیر': 596, 'عادی': 597, 'قیمتیں': 598, 'تقریباّ': 599, 'وزن': 600, 'پڑھو۔': 601, 'منہ': 602, 'زیارہ': 603, 'کرتے۔': 604, 'لڑکے': 605, 'دوں؟': 606, 'امریکہ': 607, 'فارغ': 608, 'دلچسپ': 609, 'بولنے': 610, 'اسکول': 611, 'واقعے': 612, 'سبزیاں': 613, 'بھیجی۔': 614, 'چلنا': 615, 'قسمت': 616, 'قبول': 617, 'پڑی۔': 618, 'کپ': 619, 'کرتی': 620, 'سکارف': 621, 'تمھاری': 622, 'ٹوپی': 623, 'بائسیکل': 624, 'درخواست': 625, 'ملنے': 626, 'اجلاس': 627, 'بٹایا۔': 628, 'نیچے': 629, 'گرا': 630, 'رائے': 631, 'اظہار': 632, 'فطرت': 633, 'برائی': 634, 'بندہ': 635, 'بہار': 636, 'برف': 637, 'پگھل': 638, 'چاہیئے؟': 639, 'بیواقوف': 640, 'نوکری': 641, 'پیسوں': 642, 'تمھارا': 643, 'بلیاں': 644, 'مختلف': 645, 'خیالات': 646, 'کہنے': 647, 'تھا.': 648, 'فرما': 649, 'سنتا': 650, 'بیٹھ': 651, 'کریں': 652, 'ڈھونڈو۔': 653, 'طور': 654, 'پیٹھ': 655, 'دودھ': 656, 'رک': 657, 'سن': 658, 'ہوگا،': 659, 'افریقہ': 660, 'شامل': 661, 'ضروری': 662, 'سردی': 663, 'لمبی': 664, 'پیتا': 665, 'ابتدا': 666, 'پیش': 667, 'چھت': 668, 'سہارا': 669, 'قرض': 670, 'خراب': 671, 'سنائی۔': 672, 'ناراض': 673, 'دیکھتے': 674, 'رکھو': 675, 'چلے': 676, 'دورہ': 677, 'ملتوی': 678, 'فوج': 679, 'سنیما': 680, 'چیخ': 681, 'ماری': 682, 'جامعہ': 683, 'چھوٹا': 684, 'نیلی': 685, 'باقی': 686, 'لال': 687, 'سفر': 688, 'بڑھ': 689, 'حال': 690, 'مرے': 691, 'رہی۔': 692, 'شکریہ۔': 693, 'دفتر': 694, 'طلاق': 695, 'مزہ': 696, 'آرہا': 697, 'ریستوران': 698, 'الماری': 699, 'لئیے۔': 700, 'صفحہ': 701, 'پاڑ': 702, 'پڑھتے': 703, 'کہیں': 704, 'نئے': 705, 'خامیاں': 706, 'چھٹیوں': 707, 'سلام۔': 708, 'ہوتے': 709, 'آسمان': 710, 'بادل': 711, 'سمجھ': 712, 'بولے۔': 713, 'شکل': 714, 'دیکھنا': 715, 'سکے': 716, 'ہسپانوی': 717, 'جہاں': 718, 'دینی': 719, 'اصل': 720, 'مرد': 721, 'شادی': 722, 'جوان': 723, 'بولتا': 724, 'داخل': 725, 'طالب': 726, 'قیمتی': 727, 'وسائل': 728, 'ڈھونڈا': 729, 'کیسے؟': 730, 'حل': 731, 'انکار': 732, 'بتانا': 733, 'ہاتھوں': 734, 'پکڑو۔': 735, 'ماموں': 736, 'دروازہ': 737, 'پلین': 738, 'شخص': 739, 'پوچھا': 740, 'لگتے': 741, 'خواب': 742, 'کرسی': 743, 'بینک': 744, 'بیتاب': 745, 'ریادہ': 746, 'یقین': 747, 'بوس': 748, 'بخیر۔': 749, 'معاملے': 750, 'کھڑا': 751, 'لگتا۔': 752, 'مقدپے': 753, 'بہترین': 754, 'بول': 755, 'گوشت': 756, 'آخری': 757, 'برے': 758, 'بیوی': 759, 'چاول': 760, 'تاکہ': 761, 'سکوں۔': 762, 'زنگی': 763, 'کالج': 764, 'پڑھتی': 765, 'اٹھ': 766, 'گیت': 767, 'سنا': 768, 'اٹھتے،': 769, 'نا؟': 770, 'ہیٹر': 771, 'یے۔': 772, 'ملاقات': 773, 'سزائے': 774, 'موت': 775, 'چھوٹی': 776, 'پلیٹیں': 777, 'رہے۔': 778, 'بتادو': 779, 'بچہ': 780, 'مشورہ': 781, 'دیادہ': 782, 'بور': 783, 'بچپن': 784, 'خالی': 785, 'سفید': 786, 'راستے': 787, 'پہنج': 788, 'کہا': 789, 'یار': 790, 'بولو۔': 791, 'پیشکش': 792, 'ٹھکرا': 793, 'نا۔': 794, 'بٹوہ': 795, 'کمرہ': 796, 'اپ': 797, 'ٹکٹ': 798, 'گزرے': 799, 'معلوم': 800, 'ادھر': 801, 'میز': 802, 'پڑی': 803, 'چاھتا': 804, 'حادثے': 805, 'جنگی': 806, 'تجربے': 807, 'بتایا۔': 808, 'گفتگو': 809, 'ہوں،': 810, 'چیزیں': 811, '(': 812, ')': 813, 'آئو': 814, 'عشائیہ': 815, 'چاکلیٹ': 816, 'ڈبہ': 817, 'جاننا': 818, 'پڑتا': 819, 'بچانے': 820, 'درد': 821, 'لغات': 822, 'بنانی': 823, 'دینے': 824, 'بتائو۔': 825, 'صحت': 826, 'رہتی': 827, 'آدمی': 828, 'بے': 829, 'صاف': 830, 'کيسا': 831, 'ڈرا': 832, 'انہیں': 833, 'استعفی': 834, 'کرانے': 835, 'فٹبال': 836, 'ٹیم': 837, 'لیڈ': 838, 'وایلن': 839, 'بجانا': 840, 'سیکھا؟': 841, 'خوبصورتی': 842, 'بیان': 843, 'کیوٹو': 844, 'مندر': 845, 'انگلی': 846, 'اشارہ': 847, 'تمیز': 848, 'دائرے': 849, 'واپسی': 850, 'فرمائیے۔': 851, 'دستخط': 852, 'داپس': 853, 'یہ۔': 854, 'چھٹیاں': 855, 'منانے؟': 856, 'واقعی': 857, 'اناڑی': 858, 'نا': 859, 'چاہئہے': 860, 'چلانا': 861, 'جہاز': 862, 'کشتی': 863, 'گیٹار': 864, 'ہوتا،': 865, 'لیتا۔': 866, 'دکھا': 867, 'فیصد': 868, 'بلے': 869, 'گیندیں': 870, 'ہسپتال': 871, 'تعمیر': 872, 'لندن': 873, 'شطرنج': 874, 'دیے': 875, 'چودھویں': 876, 'چاند': 877, 'نکلا۔': 878, 'مرغی': 879, 'دال': 880, 'برابر۔': 881, 'آئیے': 882, 'کریں!': 883, 'سیمسٹر': 884, 'آخرکار': 885, 'پڑندے': 886, 'پڑوا': 887, 'درکار': 888, 'چراغ': 889, 'پردوں': 890, 'لگنے': 891, 'خطرہ': 892, 'سوچتا': 893, 'گلابوں': 894, 'بیلجیم': 895, 'برا': 896, 'نوٹ': 897, 'پکرایا۔': 898, 'تمام': 899, 'مسائل': 900, 'زمہ': 901, 'دار': 902, 'ٹھہرایا': 903, 'کافی۔': 904, 'دائیں': 905, 'آستین': 906, 'چڑھائو۔': 907, 'سوچا': 908, 'اسے۔': 909, 'کیوبا': 910, 'سویا۔': 911, 'حالیہ': 912, 'محافظ': 913, 'رکتی': 914, 'فلمیں': 915, 'ناچے': 916, 'کمپیوٹر': 917, 'معلومات': 918, 'اندراج': 919, 'پاسورڈس': 920, 'دکھنے': 921, 'پڑتے': 922, 'نامہ': 923, 'قینچیاں': 924, 'ملی': 925, 'رابطہ': 926, 'ضرور': 927, 'زیادو': 928, 'ملکیوں': 929, 'پیغام': 930, 'پہنچانا': 931, 'آنکھوں': 932, 'آنسو': 933, '،': 934, 'ڈائیٹنگ': 935, 'دھونس': 936, 'جمانا': 937, 'شہریت': 938, 'کرلی۔': 939, 'ڈرامہ': 940, 'عشق': 941, 'گرفتار': 942, 'رکھا۔': 943, 'بٹن': 944, 'دبانا': 945, 'تعلق': 946, 'رکھتی': 947, 'سویٹر': 948, 'خریدا': 949, 'بار': 950, 'گندی': 951, 'اپنالی،': 952, 'چھٹے': 953, 'لو': 954, 'گروہ': 955, 'ہمایت': 956, 'دکان': 957, 'کونے': 958, 'لایا': 959, 'رہتا۔': 960, 'کلاس': 961, 'تماری': 962, 'نی': 963, 'ہیروں': 964, 'ہوئے۔': 965, 'آجائے': 966, 'پایا۔': 967, 'مجلس': 968, 'آنے': 969, 'یاد۔': 970, 'پڑھو': 971, 'حاصر': 972, 'سمجھاتا': 973, 'پیر': 974, 'مایوس': 975, 'جمیکا': 976, 'بنایا': 977, 'سیدھا': 978, 'براہ': 979, 'کرم': 980, 'دیجیے': 981, 'پیو': 982, 'زیادہ۔': 983, 'باورچی': 984, 'خانے': 985, 'اکاؤنٹس': 986, 'آڈٹ': 987, 'ازماتے': 988, 'آنا۔': 989, 'دینا۔': 990, 'روذ': 991, 'مشق': 992, 'کتوں': 993, 'الٹا': 994, 'معاف': 995, 'کیجیئے': 996, 'جتنے': 997, 'چاہئے،': 998, 'کامیابی': 999, 'پہ۔': 1000, '۱': 1001, 'میل': 1002, 'چلا۔': 1003, 'چابیاں': 1004, 'دہندگان': 1005, 'گزارش': 1006, 'آکے': 1007, 'کرائے۔': 1008, 'چلے؟': 1009, 'شرکت': 1010, 'بیج': 1011, 'پتلے': 1012, 'حیرانی': 1013, 'پرسو': 1014, 'اٹھتا': 1015, 'بیٹھتا': 1016, 'صحیص': 1017, 'پتلی': 1018, 'چھوڑنے': 1019, 'منع': 1020, 'غریب': 1021, 'سوچتے': 1022, 'ہل': 1023, 'کوٹ': 1024, 'دھویا': 1025, '.': 1026, 'پہاڑوں': 1027, 'دیئے۔': 1028, 'ریڈیو': 1029, 'بھیڑیے': 1030, 'حملہ': 1031, 'سمندری': 1032, 'اشیائیں': 1033, 'مسلئے': 1034, 'سدِ': 1035, 'باب': 1036, 'ٹائم': 1037, 'بتاہا': 1038, 'گولی': 1039, 'نکالنے': 1040, 'بھاگ': 1041, 'قتل': 1042, 'مسکراتی': 1043, 'چلنے': 1044, 'موڈ': 1045, 'غور': 1046, 'بوجھ': 1047, 'ہاتھی': 1048, 'ایشیا': 1049, 'پائے': 1050, 'منگنی': 1051, 'ہوگئی': 1052, 'بدقسمتی': 1053, 'مشکور': 1054, 'کا،': 1055, 'عقل': 1056, 'مند': 1057, 'ہوا؟': 1058, 'لکڑی': 1059, 'جل': 1060, 'چند': 1061, 'گی؟': 1062, 'کائنات': 1063, 'مس': 1064, 'توڑ': 1065, 'مڑوڑ': 1066, 'ہلکے': 1067, 'نیلے': 1068, 'پیو۔': 1069, 'چکانا': 1070, 'پہت': 1071, 'خوب۔': 1072, 'طبیعت': 1073, 'بنائی۔': 1074, 'نک': 1075, 'خاموشی': 1076, 'پہنچنا': 1077, 'مسلہ': 1078, 'موٹی': 1079, 'کل۔': 1080, 'آنکھ': 1081, 'اندھا': 1082, 'مبارک۔': 1083, 'کپروں': 1084, 'پابندی': 1085, 'موٹا': 1086, 'بہرہ': 1087, 'گی': 1088, 'خبر': 1089, 'سکھ': 1090, 'سانس': 1091, 'سکاٹ': 1092, 'لینڈ': 1093, 'جاتا۔': 1094, 'قیامت': 1095, 'سکھائی۔': 1096, 'گند': 1097, 'مچا': 1098, 'جئے': 1099, 'ہوائی': 1100, 'اڈے': 1101, 'اسلحہ': 1102, 'سکا': 1103, 'نظم': 1104, 'ترجمہ': 1105, 'قلعہ': 1106, 'کھیلتا': 1107, 'وہم': 1108, 'گانے': 1109, 'دھن': 1110, 'افسوس': 1111, 'سکوں': 1112, 'سمجھا۔': 1113, 'ہمارا': 1114, 'شمال': 1115, 'جانب': 1116, 'کود': 1117, 'یافتہ': 1118, 'پہنچے': 1119, 'افواہ': 1120, 'سچی': 1121, 'نکلی۔': 1122, 'اکیلی': 1123, 'کوتا': 1124, 'نوجوان': 1125, 'کنستر': 1126, 'سنگاپور': 1127, 'پوچھو': 1128, 'جھیل': 1129, 'تیراک': 1130, 'لنکلن': 1131, 'الیکشن': 1132, '۵۹': 1133, 'کھولو۔': 1134, 'بوریت': 1135, 'شکار': 1136, 'ہمسائیوں': 1137, 'ٹریفک': 1138, 'آیا؟': 1139, 'فوراً': 1140, 'لوٹ': 1141, 'آؤنگا': 1142, 'احساس': 1143, 'گیم': 1144, 'دیکھی۔': 1145, 'مہربانی۔': 1146, 'وہیں': 1147, 'ناول،': 1148, 'تر،': 1149, 'بورنگ': 1150, 'چاہیئے': 1151, 'تجربہ': 1152, 'ڈھونڈنے': 1153, 'مہارت': 1154, 'جانور': 1155, 'بھوک': 1156, 'کئیے۔': 1157, 'لے،': 1158, 'بڑھتا': 1159, 'ہوِئے': 1160, 'آذاد': 1161, 'جیتے': 1162, 'ہٹ': 1163, 'چاہتی۔': 1164, 'تمہارا۔': 1165, 'سہلیاں': 1166, 'سکون': 1167, 'آنا': 1168, 'کھیلوں': 1169, 'جوتے': 1170, 'سورج': 1171, 'حرارت': 1172, 'ملتی': 1173, 'دجسٹر': 1174, 'جانا۔': 1175, 'سانپ': 1176, 'وہی': 1177, 'بم': 1178, 'دھماکہ': 1179, 'حاضری': 1180, 'کرو،': 1181, 'نتیجہ': 1182, 'نکلے': 1183, 'کرلی': 1184, 'چابی': 1185, 'انٹرویو': 1186, 'دینا': 1187, 'ٹھیٹر': 1188, 'پاگل': 1189, 'مسمی': 1190, 'قیمت': 1191, '۷': 1192, 'پینس': 1193, 'علموں': 1194, '۱۸': 1195, '۲۵': 1196, 'ضائیع': 1197, 'بھاگتے': 1198, 'چور': 1199, 'لنگوٹی': 1200, 'سہی۔': 1201, 'لائنوں': 1202, 'درمیان': 1203, 'فاصلہ': 1204, 'ملو،': 1205, 'مد': 1206, 'کھڑے': 1207, 'دیکھیں۔': 1208, 'رکی': 1209, 'چکے': 1210, 'ہو،': 1211, 'قطع': 1212, 'قلامی': 1213, 'معاف۔': 1214, 'ٹھنڈا': 1215, 'سکی۔': 1216, 'عقلی': 1217, 'ظہرانے': 1218, 'بارہ': 1219, 'رہیں۔': 1220, 'بیٹا': 1221, 'سکھایا۔': 1222, 'جون': 1223, 'دھمکی': 1224, 'کارآمد': 1225, 'حامی': 1226, 'بھری۔': 1227, 'گیند': 1228, 'بھیجو۔': 1229, 'گزرا': 1230, 'آج۔': 1231, 'پڑھائی': 1232, 'دنوں': 1233, 'پزیر': 1234, 'پھپھا': 1235, 'اہم': 1236, 'محبت': 1237, 'مچھلی': 1238, 'الرجی': 1239, 'آٹھویں': 1240, 'منزل': 1241, 'ہنس': 1242, 'طافان': 1243, 'بدلاؤ': 1244, 'ڈال': 1245, 'پیوی': 1246, 'پہیلی': 1247, 'بینچ': 1248, 'لیٹا': 1249, 'بوڑھے': 1250, 'متفک': 1251, 'کتراتا': 1252, 'اتوار': 1253, 'کھیلو': 1254, 'جاننے': 1255, 'بھیجے': 1256, 'افراد': 1257, 'موٹے': 1258, 'وکیل': 1259, 'بھیجتا': 1260, 'گزاری۔': 1261, 'گلی': 1262, 'غبارے': 1263, 'پوا': 1264, 'بھری': 1265, 'مطمئن': 1266, 'پڑھای': 1267, 'مارکس': 1268, 'ذہین': 1269, 'لیکچر': 1270, 'تھورے': 1271, 'تالیاں': 1272, 'بجائی۔': 1273, 'بتاتا': 1274, 'مہینوں': 1275, 'سکو': 1276, 'فرج': 1277, 'تازہ': 1278, 'پریشان': 1279, 'ڈھول': 1280, 'سہانے۔': 1281, 'کھڑی': 1282, 'مجبوری': 1283, 'پناہ': 1284, 'گزین': 1285, 'پل': 1286, 'چھپ': 1287, 'ممالک': 1288, 'اگتا': 1289, 'عقلمند': 1290, 'لڑکا': 1291, 'باغ': 1292, 'سوچ': 1293, 'ڈوبا': 1294, 'ہوم': 1295, 'ورک': 1296, 'تھا،': 1297, 'ٹیکسنس': 1298, 'منظم': 1299, 'کھاتا': 1300, 'کھیلتی': 1301, 'پھولوں': 1302, 'سیج': 1303, 'بہتر': 1304, 'ائیرپورٹ': 1305, 'کزن': 1306, 'چھوڑنے۔': 1307, 'سچائی': 1308, 'قصبے': 1309, 'حصوں': 1310, 'تقسیم': 1311, 'نیند': 1312, 'گایا': 1313, 'لکھتا': 1314, 'گزارا۔': 1315, 'درخت': 1316, 'موقعہ': 1317, 'گنوا': 1318, 'رونے': 1319, 'سٹاپ': 1320, 'پہنچنے': 1321, 'ریا': 1322, 'تینوں': 1323, 'لڑکوں': 1324, 'کونسا': 1325, 'ڈبر': 1326, 'روٹی': 1327, 'برداشت': 1328, 'تنقید': 1329, 'سمحھ': 1330, 'طب': 1331, 'میدان': 1332, 'ترقی': 1333, 'شاندار': 1334, 'نظریں': 1335, 'جھکا': 1336, 'سودا': 1337, 'پتلون': 1338, 'مخلص': 1339, 'نقد': 1340, 'رقم': 1341, 'ادائیگی': 1342, 'کمیٹی': 1343, 'سات': 1344, 'عورتیں': 1345, 'برفباری': 1346, 'حالانکہ': 1347, 'ذخمی': 1348, 'لڑتے': 1349, 'ماضی': 1350, 'کریدنا': 1351, 'شرمندہ': 1352, 'گُم': 1353, 'ورزش': 1354, 'کرتا۔': 1355, 'وکالت': 1356, 'تربیت': 1357, 'بھاگا': 1358, 'کیوں؟': 1359, 'یونیورسٹی': 1360, 'ٹیچر': 1361, '!چلو': 1362, 'بعد،': 1363, 'کئی': 1364, 'لوٹائی۔': 1365, 'پڑتا۔': 1366, 'میچ': 1367, 'پلیز': 1368, 'آجائو۔': 1369, 'کاڑی': 1370, 'جرمن': 1371, 'لہجہ': 1372, 'پہنچتےساتھ': 1373, 'تیس': 1374, 'ہزار': 1375, 'کمائے۔': 1376, 'تھی؟': 1377, 'سٹیشن': 1378, 'دوڑے۔': 1379, 'رکنے': 1380, 'نہیں؟': 1381, '\"وہ': 1382, 'پینا': 1383, 'گا۔\"': 1384, '\"میں': 1385, 'بھی\"': 1386, 'گلاب': 1387, 'پتے': 1388, 'نازک': 1389, 'آئو؟': 1390, 'لگے': 1391, 'ڈگری': 1392, 'سینٹی': 1393, 'گریڈ': 1394, 'ابلتا': 1395, 'لنگوٹیا': 1396, 'ثابت': 1397, 'ریکیٹ': 1398, 'پین': 1399, 'گیارہ': 1400, 'بج': 1401, 'بہرے': 1402, 'اکثر': 1403, 'نشانی': 1404, 'علم': 1405, 'آندھی': 1406, 'بچھتائے': 1407, 'ہوت': 1408, 'چڑیا': 1409, 'چگ': 1410, 'کھیت۔': 1411, '۱۰': 1412, 'ڈالڑ': 1413, 'خرچ': 1414, 'لینا': 1415, 'اصولوں': 1416, 'خلاف': 1417, 'مخالف': 1418, 'آسٹریلوی': 1419, 'لڑکیاں': 1420, 'تھام': 1421, 'پہنچا': 1422, 'جلتی': 1423, 'دروازے': 1424, 'دستک': 1425, 'جگا': 1426, 'اخبار': 1427, 'روزنامہ': 1428, 'ہارڈ': 1429, 'ڈرائیو': 1430, 'تقریبا': 1431, 'بھڑھ': 1432, 'اٹھی': 1433, 'ٹائے': 1434, 'پہننے': 1435, 'وِی': 1436, 'کاافی': 1437, 'بیرا': 1438, 'ملا۔': 1439, 'پھنسا': 1440, 'صیغہ': 1441, 'سوال': 1442, 'کبھار': 1443, 'کھو': 1444, 'جانوروں': 1445, 'ڈالے۔': 1446, 'سین': 1447, 'ویکیوم': 1448, 'کلینر': 1449, 'گندا': 1450, 'ڈوبنے': 1451, 'کیلئے': 1452, 'علاوہ': 1453, 'مشہور': 1454, 'مصنف': 1455, 'بھاگنے': 1456, 'دیں۔': 1457, 'ٹوٹے': 1458, 'سکا۔': 1459, 'شہزادی': 1460, 'میک': 1461, 'لگایا': 1462, 'خربوزہ': 1463, 'خربوزے': 1464, 'پکڑتا': 1465, 'آگئی': 1466, 'مسئلہ': 1467, 'ایسے': 1468, 'گزارنا': 1469, 'پوئی': 1470, 'کسان': 1471, 'بننا': 1472, 'دکھائو۔': 1473, 'غربت': 1474, 'مجبور': 1475, 'ریڈہو': 1476, 'زندگیاں': 1477, 'بجلی': 1478, 'ہونگی؟': 1479, 'پتھر': 1480, 'دگنا': 1481, 'بھاری': 1482, 'جنونی': 1483, 'شرمیلا': 1484, 'اداس': 1485, 'عجائب': 1486, 'ڈیزائن': 1487, 'روز': 1488, 'بازار': 1489, 'سیٹ': 1490, 'آواز': 1491, 'اونچی': 1492, 'پیزہ': 1493, 'آرڈر': 1494, 'ایلبم': 1495, 'دکھائی۔': 1496, 'کھلونوں': 1497, 'کھاتے': 1498, 'رہنے': 1499}\n"
     ]
    }
   ],
   "source": [
    "print(target_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "981d9432",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train sample: ['hey,', 'look', 'at', 'this.']\n",
      "X_train_dec sample: ['<SOS>', 'یہ', 'دیکھو۔']\n",
      "y_train sample: ['یہ', 'دیکھو۔', '<EOS>']\n"
     ]
    }
   ],
   "source": [
    "train_size = int(0.9 * len(source_tokens))\n",
    "\n",
    "X_train_tokens = list(source_tokens[:train_size])\n",
    "X_val_tokens = list(source_tokens[train_size:])\n",
    "\n",
    "X_train_dec_tokens = [\n",
    "    [\"<SOS>\"] + sentence.copy()\n",
    "    for sentence in target_tokens[:train_size]\n",
    "]\n",
    "X_val_dec_tokens = [\n",
    "    [\"<SOS>\"] + sentence.copy()\n",
    "    for sentence in target_tokens[train_size:]\n",
    "]\n",
    "\n",
    "y_train_tokens = [\n",
    "    sentence.copy() + [\"<EOS>\"]\n",
    "    for sentence in target_tokens[:train_size]\n",
    "]\n",
    "y_val_tokens = [\n",
    "    sentence.copy() + [\"<EOS>\"]\n",
    "    for sentence in target_tokens[train_size:]\n",
    "]\n",
    "\n",
    "print(\"X_train sample:\", X_train_tokens[0])\n",
    "print(\"X_train_dec sample:\", X_train_dec_tokens[0])\n",
    "print(\"y_train sample:\", y_train_tokens[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c75ea3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_tokens(sentence_tokens, max_length=15):\n",
    "    padded_tokens = []\n",
    "    for tokens in sentence_tokens:\n",
    "        if len(tokens) > max_length:\n",
    "            tokens = tokens[:max_length]\n",
    "        else:\n",
    "            tokens = tokens + [\"<PAD>\"] * (max_length - len(tokens))\n",
    "        padded_tokens.append(tokens)\n",
    "    return padded_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a5df1bc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['یہ',\n",
       "  'دیکھو۔',\n",
       "  '<EOS>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>'],\n",
       " ['میں',\n",
       "  'کتا',\n",
       "  'رکھتا',\n",
       "  'ہوں۔',\n",
       "  '<EOS>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>',\n",
       "  '<PAD>']]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pad_tokens(y_train_tokens[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0a2466ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample text: [[2, 41, 1023, 0, 0, 0, 0, 0, 0, 0], [15, 8, 6, 1, 1, 0, 0, 0, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "def to_sequence(sentence_tokens, vocab, max_length=15):\n",
    "    padded_tokens = pad_tokens(sentence_tokens, max_length)\n",
    "    sequences = []\n",
    "    for tokens in padded_tokens:\n",
    "        sequence = [vocab.get(token, vocab[\"<UNK>\"]) for token in tokens]\n",
    "        sequences.append(sequence)\n",
    "    return sequences\n",
    "\n",
    "\n",
    "sample_text = [\"I am happy\", \"This is a test sentence\"]\n",
    "sample_tokens = tokenize(sample_text)\n",
    "sample_sequence = to_sequence(sample_tokens, source_vocab, max_length=10)\n",
    "print(\"Sample text:\", sample_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fbab0bed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train sample: [773, 59, 32, 191, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "y_train_dec sample: [2, 23, 298, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "y_train sample: [23, 298, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "X_train = to_sequence(X_train_tokens, source_vocab, max_length=15)\n",
    "X_val = to_sequence(X_val_tokens, source_vocab, max_length=15)\n",
    "\n",
    "X_train_dec = to_sequence(X_train_dec_tokens, target_vocab, max_length=15)\n",
    "X_val_dec = to_sequence(X_val_dec_tokens, target_vocab, max_length=15)\n",
    "\n",
    "y_train = to_sequence(y_train_tokens, target_vocab, max_length=15)\n",
    "y_val = to_sequence(y_val_tokens, target_vocab, max_length=15)\n",
    "\n",
    "\n",
    "print(\"X_train sample:\", X_train[0])\n",
    "print(\"y_train_dec sample:\", X_train_dec[0])\n",
    "print(\"y_train sample:\", y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "22f15b6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 1034\n",
      "Number of validation samples: 115\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "train_data = TensorDataset(\n",
    "    torch.tensor(X_train, dtype=torch.long),\n",
    "    torch.tensor(X_train_dec, dtype=torch.long),\n",
    "    torch.tensor(y_train, dtype=torch.long),\n",
    ")\n",
    "\n",
    "val_data = TensorDataset(\n",
    "    torch.tensor(X_val, dtype=torch.long),\n",
    "    torch.tensor(X_val_dec, dtype=torch.long),\n",
    "    torch.tensor(y_val, dtype=torch.long),\n",
    ")\n",
    "print(\"Number of training samples:\", len(train_data))\n",
    "print(\"Number of validation samples:\", len(val_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bc9d6267",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "57279f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_vocab_size,\n",
    "        embed_size,\n",
    "        hidden_size,\n",
    "        num_layers=2,\n",
    "        bidirectional=False,\n",
    "    ):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(\n",
    "            input_vocab_size, embed_size, padding_idx=0\n",
    "        )\n",
    "        self.bidirectional = bidirectional\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.gru = nn.GRU(\n",
    "            embed_size,\n",
    "            hidden_size,\n",
    "            num_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=bidirectional,\n",
    "            dropout=0.5 if num_layers > 1 else 0,\n",
    "        )\n",
    "        self.embed_dropout = nn.Dropout(0.5)\n",
    "        if bidirectional:\n",
    "            self.hidden_projection = nn.Linear(hidden_size * 2, hidden_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embed_dropout(self.embedding(x))\n",
    "        outputs, hidden = self.gru(embedded)\n",
    "        if self.bidirectional:\n",
    "            # Reshape: (num_layers * 2, batch, hidden) -> (num_layers, 2, batch, hidden)\n",
    "            hidden = hidden.view(self.num_layers, 2, -1, self.hidden_size)\n",
    "            # Concatenate forward and backward\n",
    "            hidden = torch.cat([hidden[:, 0, :, :], hidden[:, 1, :, :]], dim=2)\n",
    "            # Project to decoder size\n",
    "            hidden = self.hidden_projection(hidden)\n",
    "        return outputs, hidden\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_vocab_size, embed_size, hidden_size, num_layers=2):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(\n",
    "            output_vocab_size, embed_size, padding_idx=0\n",
    "        )\n",
    "        self.gru = nn.GRU(\n",
    "            embed_size,\n",
    "            hidden_size,\n",
    "            num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=0.5 if num_layers > 1 else 0,\n",
    "        )\n",
    "        self.embed_dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        embedded = self.embed_dropout(self.embedding(x))\n",
    "        outputs, hidden = self.gru(embedded, hidden)\n",
    "        return outputs, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "33487ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDecoder(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(EncoderDecoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.fc = nn.Linear(decoder.hidden_size, len(target_vocab))\n",
    "        self.dropout = nn.Dropout(0.6)\n",
    "\n",
    "    def forward(self, source, target):\n",
    "        _, encoder_hidden = self.encoder(source)\n",
    "        decoder_outputs, _ = self.decoder(target, encoder_hidden)\n",
    "        decoder_outputs = self.dropout(decoder_outputs)\n",
    "        return self.fc(decoder_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae29cd99",
   "metadata": {},
   "source": [
    "# Training and Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "000a5fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "\n",
    "\n",
    "def train_epoch(model, dataloader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    progress_bar = tqdm.tqdm(dataloader, desc=\"Training\")\n",
    "    for enc_inputs, dec_inputs, targets in progress_bar:\n",
    "        # Move data to device\n",
    "        enc_inputs, dec_inputs, targets = (\n",
    "            enc_inputs.to(device),\n",
    "            dec_inputs.to(device),\n",
    "            targets.to(device),\n",
    "        )\n",
    "\n",
    "        outputs = model(enc_inputs, dec_inputs)\n",
    "        outputs = outputs.reshape(-1, outputs.size(-1))\n",
    "        targets = targets.reshape(-1)\n",
    "\n",
    "        loss = criterion(outputs, targets)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        # Calculate accuracy\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        non_pad_mask = targets != 0  # Create mask for non-padding tokens\n",
    "        total += non_pad_mask.sum().item()\n",
    "        correct += ((predicted == targets) & non_pad_mask).sum().item()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Update progress bar\n",
    "        progress_bar.set_postfix(\n",
    "            {\"loss\": f\"{loss.item():.4f}\", \"acc\": f\"{100 * correct / total:.2f}%\"}\n",
    "        )\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = 100 * correct / total\n",
    "\n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132a0aad",
   "metadata": {},
   "source": [
    "## Step 11: Validation Function\n",
    "\n",
    "The validation function evaluates the model without updating weights. This helps us monitor overfitting and select the best model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "fe1ea651",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    progress_bar = tqdm.tqdm(dataloader, desc=\"Validating\")\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for enc_inputs, dec_inputs, targets in progress_bar:\n",
    "            # Move data to device\n",
    "            enc_inputs, dec_inputs, targets = (\n",
    "                enc_inputs.to(device),\n",
    "                dec_inputs.to(device),\n",
    "                targets.to(device),\n",
    "            )\n",
    "\n",
    "            outputs = model(enc_inputs, dec_inputs)\n",
    "            outputs = outputs.reshape(-1, outputs.size(-1))\n",
    "            targets = targets.reshape(-1)\n",
    "\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            # Calculate accuracy\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            non_pad_mask = targets != 0  # Create mask for non-padding tokens\n",
    "            total += non_pad_mask.sum().item()\n",
    "            correct += ((predicted == targets) & non_pad_mask).sum().item()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Update progress bar with current accuracy\n",
    "            progress_bar.set_postfix(\n",
    "                {\"loss\": f\"{loss.item():.4f}\", \"acc\": f\"{100 * correct / total:.2f}%\"}\n",
    "            )\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = 100 * correct / total\n",
    "\n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "8e343101",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EncoderDecoder(\n",
       "  (encoder): Encoder(\n",
       "    (embedding): Embedding(1500, 256, padding_idx=0)\n",
       "    (gru): GRU(256, 512, num_layers=2, batch_first=True, dropout=0.5, bidirectional=True)\n",
       "    (embed_dropout): Dropout(p=0.5, inplace=False)\n",
       "    (hidden_projection): Linear(in_features=1024, out_features=512, bias=True)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (embedding): Embedding(1500, 256, padding_idx=0)\n",
       "    (gru): GRU(256, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
       "    (embed_dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (fc): Linear(in_features=512, out_features=1500, bias=True)\n",
       "  (dropout): Dropout(p=0.6, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate the models\n",
    "embed_size = 256\n",
    "hidden_size = 512\n",
    "num_layers = 2\n",
    "encoder = Encoder(\n",
    "    len(source_vocab), embed_size, hidden_size, num_layers, bidirectional=True\n",
    ")\n",
    "decoder = Decoder(len(target_vocab), embed_size, hidden_size, num_layers)\n",
    "\n",
    "model = EncoderDecoder(encoder, decoder)\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a4bbe4db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "EncoderDecoder                           [1, 15, 1500]             --\n",
       "├─Encoder: 1-1                           [1, 15, 1024]             --\n",
       "│    └─Embedding: 2-1                    [1, 15, 256]              384,000\n",
       "│    └─Dropout: 2-2                      [1, 15, 256]              --\n",
       "│    └─GRU: 2-3                          [1, 15, 1024]             7,090,176\n",
       "│    └─Linear: 2-4                       [2, 1, 512]               524,800\n",
       "├─Decoder: 1-2                           [1, 15, 512]              --\n",
       "│    └─Embedding: 2-5                    [1, 15, 256]              384,000\n",
       "│    └─Dropout: 2-6                      [1, 15, 256]              --\n",
       "│    └─GRU: 2-7                          [1, 15, 512]              2,758,656\n",
       "├─Dropout: 1-3                           [1, 15, 512]              --\n",
       "├─Linear: 1-4                            [1, 15, 1500]             769,500\n",
       "==========================================================================================\n",
       "Total params: 11,911,132\n",
       "Trainable params: 11,911,132\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 150.32\n",
       "==========================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 0.43\n",
       "Params size (MB): 47.64\n",
       "Estimated Total Size (MB): 48.08\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "input_sample = torch.zeros((1, 15), dtype=torch.long).to(device)\n",
    "summary(model, input_data=(input_sample, input_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c5eba591",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training batches: 17\n",
      "Number of validation batches: 2\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "# Loss function (ignore padding tokens)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "\n",
    "# Optimizer\n",
    "learning_rate = 0.0007\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode=\"min\", factor=0.5, patience=5\n",
    ")\n",
    "\n",
    "print(f\"Number of training batches: {len(train_loader)}\")\n",
    "print(f\"Number of validation batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "e47ded53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Device: cuda\n",
      "Number of epochs: 150\n",
      "\n",
      "Epoch 1/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 17/17 [00:01<00:00, 16.86it/s, loss=5.4288, acc=11.10%]\n",
      "Validating: 100%|██████████| 2/2 [00:00<00:00, 46.48it/s, loss=5.2349, acc=14.41%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ Saved best model (Val Acc: 14.41%)\n",
      "Epoch 2/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 17/17 [00:00<00:00, 17.92it/s, loss=4.6475, acc=16.08%]\n",
      "Validating: 100%|██████████| 2/2 [00:00<00:00, 45.91it/s, loss=5.0170, acc=14.62%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ Saved best model (Val Acc: 14.62%)\n",
      "Epoch 3/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 17/17 [00:01<00:00, 16.48it/s, loss=5.1446, acc=17.64%]\n",
      "Validating: 100%|██████████| 2/2 [00:00<00:00, 34.61it/s, loss=4.8138, acc=18.51%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ Saved best model (Val Acc: 18.51%)\n",
      "Epoch 4/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 17/17 [00:00<00:00, 17.38it/s, loss=5.1359, acc=19.21%]\n",
      "Validating: 100%|██████████| 2/2 [00:00<00:00, 46.98it/s, loss=4.7100, acc=20.08%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ Saved best model (Val Acc: 20.08%)\n",
      "Epoch 5/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 17/17 [00:00<00:00, 19.70it/s, loss=4.9439, acc=20.98%]\n",
      "Validating: 100%|██████████| 2/2 [00:00<00:00, 37.98it/s, loss=4.6172, acc=19.98%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 17/17 [00:00<00:00, 20.52it/s, loss=4.7027, acc=22.27%]\n",
      "Validating: 100%|██████████| 2/2 [00:00<00:00, 43.88it/s, loss=4.4472, acc=23.87%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ Saved best model (Val Acc: 23.87%)\n",
      "Epoch 7/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 17/17 [00:00<00:00, 21.09it/s, loss=4.1918, acc=24.16%]\n",
      "Validating: 100%|██████████| 2/2 [00:00<00:00, 39.08it/s, loss=4.3547, acc=23.66%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 17/17 [00:00<00:00, 19.07it/s, loss=4.2706, acc=25.81%]\n",
      "Validating: 100%|██████████| 2/2 [00:00<00:00, 44.58it/s, loss=4.2226, acc=26.39%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ Saved best model (Val Acc: 26.39%)\n",
      "Epoch 9/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 17/17 [00:00<00:00, 18.41it/s, loss=3.7477, acc=27.26%]\n",
      "Validating: 100%|██████████| 2/2 [00:00<00:00, 42.97it/s, loss=4.1901, acc=27.66%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ Saved best model (Val Acc: 27.66%)\n",
      "Epoch 10/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 17/17 [00:00<00:00, 19.21it/s, loss=3.3512, acc=28.82%]\n",
      "Validating: 100%|██████████| 2/2 [00:00<00:00, 34.47it/s, loss=4.0599, acc=30.70%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ Saved best model (Val Acc: 30.70%)\n",
      "Epoch 11/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 17/17 [00:01<00:00, 15.04it/s, loss=2.9100, acc=30.22%]\n",
      "Validating: 100%|██████████| 2/2 [00:00<00:00, 28.77it/s, loss=4.0883, acc=28.60%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 17/17 [00:01<00:00, 15.61it/s, loss=3.7075, acc=31.80%]\n",
      "Validating: 100%|██████████| 2/2 [00:00<00:00, 25.26it/s, loss=3.9668, acc=31.13%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ Saved best model (Val Acc: 31.13%)\n",
      "Epoch 13/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 17/17 [00:01<00:00, 15.00it/s, loss=4.0663, acc=32.99%]\n",
      "Validating: 100%|██████████| 2/2 [00:00<00:00, 20.78it/s, loss=3.9230, acc=31.34%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ Saved best model (Val Acc: 31.34%)\n",
      "Epoch 14/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 17/17 [00:01<00:00, 16.14it/s, loss=3.3972, acc=35.29%]\n",
      "Validating: 100%|██████████| 2/2 [00:00<00:00, 29.41it/s, loss=3.8510, acc=32.28%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ Saved best model (Val Acc: 32.28%)\n",
      "Epoch 15/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 17/17 [00:00<00:00, 17.42it/s, loss=3.6718, acc=36.97%]\n",
      "Validating: 100%|██████████| 2/2 [00:00<00:00, 46.90it/s, loss=3.9513, acc=31.44%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 17/17 [00:01<00:00, 16.59it/s, loss=3.2455, acc=38.53%]\n",
      "Validating: 100%|██████████| 2/2 [00:00<00:00, 44.72it/s, loss=3.7577, acc=33.54%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ Saved best model (Val Acc: 33.54%)\n",
      "Epoch 17/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 17/17 [00:01<00:00, 16.62it/s, loss=2.7499, acc=41.14%]\n",
      "Validating: 100%|██████████| 2/2 [00:00<00:00, 35.34it/s, loss=3.7840, acc=35.23%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ Saved best model (Val Acc: 35.23%)\n",
      "Epoch 18/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 17/17 [00:01<00:00, 16.61it/s, loss=2.4921, acc=42.64%]\n",
      "Validating: 100%|██████████| 2/2 [00:00<00:00, 40.20it/s, loss=3.7391, acc=35.54%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ Saved best model (Val Acc: 35.54%)\n",
      "Epoch 19/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 17/17 [00:00<00:00, 17.27it/s, loss=2.6860, acc=45.10%]\n",
      "Validating: 100%|██████████| 2/2 [00:00<00:00, 30.79it/s, loss=3.7529, acc=36.17%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ Saved best model (Val Acc: 36.17%)\n",
      "Epoch 20/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 17/17 [00:01<00:00, 16.47it/s, loss=2.4139, acc=46.27%]\n",
      "Validating: 100%|██████████| 2/2 [00:00<00:00, 34.23it/s, loss=3.7813, acc=36.38%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ Saved best model (Val Acc: 36.38%)\n",
      "Epoch 21/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 17/17 [00:00<00:00, 18.55it/s, loss=2.6710, acc=48.79%]\n",
      "Validating: 100%|██████████| 2/2 [00:00<00:00, 34.27it/s, loss=3.8153, acc=37.12%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ Saved best model (Val Acc: 37.12%)\n",
      "Epoch 22/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 17/17 [00:01<00:00, 16.00it/s, loss=2.2600, acc=50.71%]\n",
      "Validating: 100%|██████████| 2/2 [00:00<00:00, 20.33it/s, loss=3.7166, acc=39.12%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ Saved best model (Val Acc: 39.12%)\n",
      "Epoch 23/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 17/17 [00:00<00:00, 19.05it/s, loss=2.4526, acc=52.26%]\n",
      "Validating: 100%|██████████| 2/2 [00:00<00:00, 34.86it/s, loss=3.7148, acc=37.85%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 17/17 [00:00<00:00, 19.10it/s, loss=2.0605, acc=53.25%]\n",
      "Validating: 100%|██████████| 2/2 [00:00<00:00, 39.37it/s, loss=3.7596, acc=38.91%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 17/17 [00:01<00:00, 16.57it/s, loss=1.5992, acc=57.45%]\n",
      "Validating: 100%|██████████| 2/2 [00:00<00:00, 45.72it/s, loss=3.7371, acc=40.17%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ Saved best model (Val Acc: 40.17%)\n",
      "Epoch 26/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 17/17 [00:00<00:00, 20.33it/s, loss=1.9008, acc=58.94%]\n",
      "Validating: 100%|██████████| 2/2 [00:00<00:00, 31.40it/s, loss=3.7388, acc=40.38%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ Saved best model (Val Acc: 40.38%)\n",
      "Epoch 27/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 17/17 [00:00<00:00, 18.18it/s, loss=1.4416, acc=60.27%]\n",
      "Validating: 100%|██████████| 2/2 [00:00<00:00, 34.74it/s, loss=3.7192, acc=41.43%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ Saved best model (Val Acc: 41.43%)\n",
      "Epoch 28/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 17/17 [00:00<00:00, 20.45it/s, loss=1.6744, acc=61.44%]\n",
      "Validating: 100%|██████████| 2/2 [00:00<00:00, 28.17it/s, loss=3.6972, acc=40.90%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 17/17 [00:00<00:00, 20.13it/s, loss=1.9789, acc=62.60%]\n",
      "Validating: 100%|██████████| 2/2 [00:00<00:00, 37.93it/s, loss=3.7527, acc=40.69%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 17/17 [00:00<00:00, 17.18it/s, loss=1.8314, acc=63.10%]\n",
      "Validating: 100%|██████████| 2/2 [00:00<00:00, 44.39it/s, loss=3.7570, acc=41.32%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 17/17 [00:00<00:00, 20.47it/s, loss=1.2125, acc=64.26%]\n",
      "Validating: 100%|██████████| 2/2 [00:00<00:00, 48.79it/s, loss=3.7403, acc=41.54%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ Saved best model (Val Acc: 41.54%)\n",
      "Epoch 32/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 17/17 [00:00<00:00, 17.58it/s, loss=1.5174, acc=65.06%]\n",
      "Validating: 100%|██████████| 2/2 [00:00<00:00, 23.37it/s, loss=3.7679, acc=41.85%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ Saved best model (Val Acc: 41.85%)\n",
      "Epoch 33/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 17/17 [00:00<00:00, 21.29it/s, loss=1.3556, acc=66.02%]\n",
      "Validating: 100%|██████████| 2/2 [00:00<00:00, 40.47it/s, loss=3.7744, acc=41.54%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 17/17 [00:00<00:00, 21.02it/s, loss=1.4578, acc=66.11%]\n",
      "Validating: 100%|██████████| 2/2 [00:00<00:00, 28.13it/s, loss=3.7871, acc=41.85%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 17/17 [00:00<00:00, 20.38it/s, loss=1.3376, acc=66.40%]\n",
      "Validating: 100%|██████████| 2/2 [00:00<00:00, 33.64it/s, loss=3.8029, acc=41.43%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 17/17 [00:00<00:00, 22.25it/s, loss=1.6650, acc=67.68%]\n",
      "Validating: 100%|██████████| 2/2 [00:00<00:00, 48.30it/s, loss=3.8241, acc=41.75%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 17/17 [00:00<00:00, 21.51it/s, loss=1.2242, acc=68.20%]\n",
      "Validating: 100%|██████████| 2/2 [00:00<00:00, 46.03it/s, loss=3.8126, acc=41.85%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 17/17 [00:00<00:00, 21.73it/s, loss=1.2004, acc=67.89%]\n",
      "Validating: 100%|██████████| 2/2 [00:00<00:00, 48.65it/s, loss=3.8211, acc=41.85%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 17/17 [00:00<00:00, 22.43it/s, loss=1.4818, acc=68.03%]\n",
      "Validating: 100%|██████████| 2/2 [00:00<00:00, 51.40it/s, loss=3.8211, acc=41.75%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 17/17 [00:00<00:00, 20.81it/s, loss=1.1035, acc=68.89%]\n",
      "Validating: 100%|██████████| 2/2 [00:00<00:00, 47.65it/s, loss=3.8332, acc=42.17%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ Saved best model (Val Acc: 42.17%)\n",
      "Epoch 41/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 17/17 [00:00<00:00, 21.87it/s, loss=1.2818, acc=68.92%]\n",
      "Validating: 100%|██████████| 2/2 [00:00<00:00, 46.14it/s, loss=3.8212, acc=42.17%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 17/17 [00:00<00:00, 19.18it/s, loss=1.0286, acc=69.42%]\n",
      "Validating: 100%|██████████| 2/2 [00:00<00:00, 59.82it/s, loss=3.8404, acc=41.85%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 17/17 [00:00<00:00, 22.40it/s, loss=1.4923, acc=69.28%]\n",
      "Validating: 100%|██████████| 2/2 [00:00<00:00, 50.76it/s, loss=3.8408, acc=42.17%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 17/17 [00:00<00:00, 22.16it/s, loss=1.1985, acc=69.68%]\n",
      "Validating: 100%|██████████| 2/2 [00:00<00:00, 46.06it/s, loss=3.8479, acc=41.85%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 17/17 [00:00<00:00, 21.20it/s, loss=1.3202, acc=70.16%]\n",
      "Validating: 100%|██████████| 2/2 [00:00<00:00, 52.83it/s, loss=3.8460, acc=41.75%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 17/17 [00:00<00:00, 22.33it/s, loss=1.2012, acc=69.96%]\n",
      "Validating: 100%|██████████| 2/2 [00:00<00:00, 45.75it/s, loss=3.8540, acc=41.96%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 17/17 [00:00<00:00, 20.98it/s, loss=1.1938, acc=70.48%]\n",
      "Validating: 100%|██████████| 2/2 [00:00<00:00, 42.66it/s, loss=3.8464, acc=41.85%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 17/17 [00:00<00:00, 22.73it/s, loss=1.2217, acc=70.21%]\n",
      "Validating: 100%|██████████| 2/2 [00:00<00:00, 40.95it/s, loss=3.8485, acc=41.64%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 17/17 [00:00<00:00, 22.52it/s, loss=1.1190, acc=70.60%]\n",
      "Validating: 100%|██████████| 2/2 [00:00<00:00, 44.62it/s, loss=3.8517, acc=41.75%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 17/17 [00:00<00:00, 20.60it/s, loss=0.9606, acc=70.14%]\n",
      "Validating: 100%|██████████| 2/2 [00:00<00:00, 46.48it/s, loss=3.8543, acc=41.96%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 17/17 [00:00<00:00, 22.72it/s, loss=1.2703, acc=70.55%]\n",
      "Validating: 100%|██████████| 2/2 [00:00<00:00, 51.40it/s, loss=3.8557, acc=41.96%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 17/17 [00:00<00:00, 18.99it/s, loss=0.8996, acc=70.28%]\n",
      "Validating: 100%|██████████| 2/2 [00:00<00:00, 42.24it/s, loss=3.8553, acc=41.75%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 17/17 [00:00<00:00, 18.35it/s, loss=1.2105, acc=70.88%]\n",
      "Validating: 100%|██████████| 2/2 [00:00<00:00, 54.25it/s, loss=3.8607, acc=41.96%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 17/17 [00:00<00:00, 18.23it/s, loss=1.3192, acc=71.50%]\n",
      "Validating: 100%|██████████| 2/2 [00:00<00:00, 53.00it/s, loss=3.8667, acc=42.17%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 17/17 [00:01<00:00, 16.98it/s, loss=1.1098, acc=70.74%]\n",
      "Validating: 100%|██████████| 2/2 [00:00<00:00, 48.15it/s, loss=3.8690, acc=42.06%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping triggered after 55 epochs\n",
      "Training complete!\n",
      "Best validation accuracy: 42.17%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 150\n",
    "best_val_acc = 0.0\n",
    "patience = 15\n",
    "patience_counter = 0\n",
    "\n",
    "# Track training history\n",
    "train_losses = []\n",
    "train_accs = []\n",
    "val_losses = []\n",
    "val_accs = []\n",
    "\n",
    "print(\"Starting training...\")\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"Number of epochs: {num_epochs}\\n\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "    # Train\n",
    "    train_loss, train_acc = train_epoch(\n",
    "        model, train_loader, criterion, optimizer, device\n",
    "    )\n",
    "    # Validate\n",
    "    val_loss, val_acc = validate(model, val_loader, criterion, device)\n",
    "\n",
    "    # Track history\n",
    "    train_losses.append(train_loss)\n",
    "    train_accs.append(train_acc)\n",
    "    val_losses.append(val_loss)\n",
    "    val_accs.append(val_acc)\n",
    "    \n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    # Save best model\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        patience_counter = 0\n",
    "        torch.save(model.state_dict(), \"nmt_model.pth\")\n",
    "        print(f\"  ✓ Saved best model (Val Acc: {val_acc:.2f}%)\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping triggered after {epoch + 1} epochs\")\n",
    "            break\n",
    "\n",
    "print(\"Training complete!\")\n",
    "print(f\"Best validation accuracy: {best_val_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6cda3b4",
   "metadata": {},
   "source": [
    "# Load Best Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "a68efb4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 2/2 [00:00<00:00, 12.77it/s, loss=3.8332, acc=42.17%]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3.904155969619751, 42.16614090431125)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"nmt_model.pth\", weights_only=True))\n",
    "model = model.to(device)\n",
    "validate(model, val_loader, criterion, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "1cae50ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing translations:\n",
      "\n",
      "Source: Hey, look at this.\n",
      "Target: یہ دیکھو۔\n",
      "\n",
      "Source: I keep a dog.\n",
      "Target: میں کتا رکھتا ہوں۔\n",
      "\n",
      "Source: I just don't know what to say.\n",
      "Target: مجھے پتا ہے کہ تم امیر ہو۔\n",
      "\n",
      "Source: How is it going?\n",
      "Target: کيسا چل رہا ہے ؟\n",
      "\n",
      "Source: Trust me!\n",
      "Target: مجھ پر بھروسہ\n",
      "\n",
      "Source: Tom's scared.\n",
      "Target: ٹام مر ہے۔\n",
      "\n",
      "Source: I'm seeing them tonight.\n",
      "Target: مجھے نیند سے الرجی ہے۔\n",
      "\n",
      "Source: What did she do today?\n",
      "Target: انہیوں نے آج کیا کیا؟\n",
      "\n",
      "Source: He decided to submit his resignation.\n",
      "Target: اس نے اپنا استعفی کرانے کرانے\n",
      "\n",
      "Source: His car is two years old.\n",
      "Target: اس کی گاڑی دو پرانی ہے۔\n",
      "\n"
     ]
    }
   ],
   "source": [
    "target_i2t = {idx: token for token, idx in target_vocab.items()}\n",
    "\n",
    "\n",
    "def translate(sentence, model, source_vocab, target_vocab, max_length=20, device=\"cpu\"):\n",
    "    model.eval()\n",
    "    sentence_tokens = sentence.lower().split()\n",
    "    sequence = to_sequence([sentence_tokens], source_vocab, max_length=max_length)\n",
    "    encoder_input = torch.tensor(sequence, dtype=torch.long).to(device)\n",
    "\n",
    "    # Start with <SOS> token\n",
    "    decoder_input = [target_vocab[\"<SOS>\"]]\n",
    "    translation = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        _, encoder_hidden = model.encoder(encoder_input)\n",
    "\n",
    "        for _ in range(max_length):\n",
    "            # Prepare decoder input\n",
    "            dec_input = torch.tensor([decoder_input], dtype=torch.long).to(device)\n",
    "\n",
    "            # Decode\n",
    "            decoder_outputs, _ = model.decoder(dec_input, encoder_hidden)\n",
    "\n",
    "            # Get prediction for the last token\n",
    "            output = model.fc(decoder_outputs[:, -1, :])\n",
    "            predicted_id = output.argmax(dim=-1).item()\n",
    "\n",
    "            # Check for EOS token\n",
    "            if predicted_id == target_vocab[\"<EOS>\"]:\n",
    "                break\n",
    "\n",
    "            # Get the predicted word\n",
    "            predicted_word = target_i2t.get(predicted_id, \"\")\n",
    "\n",
    "            # Skip special tokens in output\n",
    "            if predicted_word not in [\"<PAD>\", \"<UNK>\", \"<SOS>\", \"<EOS>\"]:\n",
    "                translation.append(predicted_word)\n",
    "\n",
    "            # Add predicted token to decoder input for next iteration\n",
    "            decoder_input.append(predicted_id)\n",
    "    return \" \".join(translation)\n",
    "\n",
    "\n",
    "# Test the translation function\n",
    "test_sentences = source_sentences[:10]\n",
    "\n",
    "print(\"Testing translations:\\n\")\n",
    "for sentence in test_sentences:\n",
    "    translated = translate(sentence, model, source_vocab, target_vocab, device=device)\n",
    "    print(f\"Source: {sentence}\")\n",
    "    print(f\"Target: {translated}\")\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ul-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
