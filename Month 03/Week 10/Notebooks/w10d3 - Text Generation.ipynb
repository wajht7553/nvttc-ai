{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "445d3b95",
   "metadata": {},
   "source": [
    "# Summer of Code - Artificial Intelligence\n",
    "\n",
    "## Week 10: Deep Learning\n",
    "\n",
    "### Day 03: Text Generation\n",
    "\n",
    "In this notebook, we will explore text generation, a **Sequence to Sequence** task, using RNNs with GRUs layers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b422af",
   "metadata": {},
   "source": [
    "# Shakespeare's Works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "470d20a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Shakespeare's works...\n",
      "Download complete!\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlretrieve\n",
    "\n",
    "shakespeare_url = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "print(\"Downloading Shakespeare's works...\")\n",
    "\n",
    "urlretrieve(shakespeare_url, \"shakespeare.txt\")\n",
    "print(\"Download complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08bd1f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the text file\n",
    "with open(\"shakespeare.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    shakespeare_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0815061a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1115394"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(shakespeare_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b688d877",
   "metadata": {},
   "source": [
    "## Vocabulary Creation\n",
    "\n",
    "For a character-level language model like this, our vocabulary consists of all unique characters present in the text. This includes letters, punctuation marks, spaces, and special characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "528f9daf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' ', '!', 'd', 'e', 'h', 'l', 'o', 'r', 'w']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Hello world!\"\n",
    "text_tokens = sorted(set(text.lower()))\n",
    "text_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cca095cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_to_id = {ch: i for i, ch in enumerate(text_tokens)}\n",
    "text_to_id['l']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d5b36e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'e'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_to_text = {i: ch for ch, i in text_to_id.items()}\n",
    "id_to_text[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6830e07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique characters: 39\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(shakespeare_text.lower())))\n",
    "n_tokens = len(chars)\n",
    "\n",
    "print(f\"Number of unique characters: {n_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9aba2128",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create character-to-index and index-to-character mappings\n",
    "char_to_idx = {char: idx for idx, char in enumerate(chars)}\n",
    "idx_to_char = {idx: char for char, idx in char_to_idx.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4777eb64",
   "metadata": {},
   "source": [
    "## Encoding Sequences\n",
    "We will convert the entire text into a sequence of character IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c7901632",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first citi\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[18, 21, 30, 31, 32, 1, 15, 21, 32, 21]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded = [char_to_idx[char] for char in shakespeare_text.lower()]\n",
    "print(shakespeare_text.lower()[:10])\n",
    "encoded[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb36a071",
   "metadata": {},
   "source": [
    "# Text Generation Dataset\n",
    "\n",
    "To train our model, we need to create input-target pairs.\n",
    "- The input is a sequence of characters.\n",
    "- The output is also a sequence of characters, but shifted one position to the right of the input position.\n",
    "\n",
    "**For example:**\\\n",
    "Text: \"Shakespeare\"\\\n",
    "Sequence Length (Context Window): 5\n",
    "\n",
    "Input: \"Shake\"\\\n",
    "Target: \"haksp\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7333ea34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class CharDataset(Dataset):\n",
    "    def __init__(self, encoded_text, window_length):\n",
    "        self.encoded_text = encoded_text\n",
    "        self.window_length = window_length\n",
    "\n",
    "        # Create all possible windows\n",
    "        self.windows = []\n",
    "        for i in range(len(encoded_text) - window_length):\n",
    "            window = encoded_text[i : i + window_length + 1]  # +1 for target\n",
    "            self.windows.append(window)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.windows)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        window = self.windows[idx]\n",
    "        # Input: all characters except the last\n",
    "        # Target: all characters except the first (shifted by 1)\n",
    "        input_seq = window[:-1]\n",
    "        target_seq = window[1:]\n",
    "\n",
    "        return input_seq, target_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c42a4d95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[18, 21, 30, 31, 32, 1, 15, 21, 32, 21]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f66485e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[18, 21, 30, 31],\n",
       " [21, 30, 31, 32],\n",
       " [30, 31, 32, 1],\n",
       " [31, 32, 1, 15],\n",
       " [32, 1, 15, 21]]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_dataset = CharDataset(encoded, window_length=3)\n",
    "dummy_dataset.windows[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "63860f64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([18, 21, 30], [21, 30, 31])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a811892",
   "metadata": {},
   "source": [
    "## Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4b47a17f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 1,000,000 characters\n",
      "Validation set size: 60,000 characters\n",
      "Test set size: 55,394 characters\n"
     ]
    }
   ],
   "source": [
    "# Set window length\n",
    "window_length = 200\n",
    "train_size = 1_000_000\n",
    "valid_size = 60_000\n",
    "\n",
    "encoded_tensor = torch.tensor(encoded, dtype=torch.long)\n",
    "train_encoded = encoded_tensor[:train_size]\n",
    "valid_encoded = encoded_tensor[train_size : train_size + valid_size]\n",
    "test_encoded = encoded_tensor[train_size + valid_size :]\n",
    "\n",
    "print(f\"Training set size: {len(train_encoded):,} characters\")\n",
    "print(f\"Validation set size: {len(valid_encoded):,} characters\")\n",
    "print(f\"Test set size: {len(test_encoded):,} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f0788425",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training windows: 999,800\n",
      "Validation windows: 59,800\n",
      "Test windows: 55,194\n"
     ]
    }
   ],
   "source": [
    "# Create datasets\n",
    "train_dataset = CharDataset(train_encoded, window_length)\n",
    "valid_dataset = CharDataset(valid_encoded, window_length)\n",
    "test_dataset = CharDataset(test_encoded, window_length)\n",
    "\n",
    "print(f\"\\nTraining windows: {len(train_dataset):,}\")\n",
    "print(f\"Validation windows: {len(valid_dataset):,}\")\n",
    "print(f\"Test windows: {len(test_dataset):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fbc4def",
   "metadata": {},
   "source": [
    "## DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0bca2b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=0,  # Set to 0 for Windows compatibility\n",
    "    pin_memory=True if torch.cuda.is_available() else False,\n",
    ")\n",
    "\n",
    "valid_loader = DataLoader(\n",
    "    valid_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    pin_memory=True if torch.cuda.is_available() else False,\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    pin_memory=True if torch.cuda.is_available() else False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d6825f",
   "metadata": {},
   "source": [
    "# Char-RNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "01b62927",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "\n",
    "class CharRNN(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self, vocab_size, embed_dim=256, hidden_dim=512, num_layers=3, dropout=0.3\n",
    "    ):\n",
    "        super(CharRNN, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.embed_dropout = nn.Dropout(0.2)\n",
    "        self.gru = nn.GRU(\n",
    "            embed_dim,\n",
    "            hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "        )\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "        \n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        \"\"\"Initialize weights with better defaults\"\"\"\n",
    "        nn.init.uniform_(self.embedding.weight, -0.1, 0.1)\n",
    "        nn.init.xavier_uniform_(self.fc.weight)\n",
    "        nn.init.zeros_(self.fc.bias)\n",
    "\n",
    "    def forward(self, x):  # x shape: [batch_size, sequence_length]\n",
    "        embedded = self.embedding(x)\n",
    "        embedded = self.embed_dropout(embedded)\n",
    "        gru_out, hidden = self.gru(embedded)\n",
    "        gru_out = self.dropout(gru_out)\n",
    "        output = self.fc(gru_out)\n",
    "        return output, hidden\n",
    "\n",
    "\n",
    "model = CharRNN(\n",
    "    vocab_size=n_tokens,\n",
    "    embed_dim=128,\n",
    "    hidden_dim=256,\n",
    "    num_layers=2,\n",
    "    dropout=0.3,\n",
    ")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b6d4def8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "CharRNN                                  [1, 200, 39]              --\n",
       "├─Embedding: 1-1                         [1, 200, 128]             4,992\n",
       "├─Dropout: 1-2                           [1, 200, 128]             --\n",
       "├─GRU: 1-3                               [1, 200, 256]             691,200\n",
       "├─Dropout: 1-4                           [1, 200, 256]             --\n",
       "├─Linear: 1-5                            [1, 200, 39]              10,023\n",
       "==========================================================================================\n",
       "Total params: 706,215\n",
       "Trainable params: 706,215\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 138.26\n",
       "==========================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 0.68\n",
       "Params size (MB): 2.82\n",
       "Estimated Total Size (MB): 3.50\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "input_sample = torch.zeros((1, window_length), dtype=torch.long).to(device)\n",
    "summary(model, input_data=input_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e76a68",
   "metadata": {},
   "source": [
    "## Loss Function and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a18f386",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode=\"min\", factor=0.5, patience=2, min_lr=1e-6\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0f6ee3",
   "metadata": {},
   "source": [
    "# Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "06472002",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "\n",
    "\n",
    "def train_epoch(model, dataloader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    progress_bar = tqdm.tqdm(dataloader, desc=\"Training\")\n",
    "    for batch_idx, (inputs, targets) in enumerate(progress_bar):\n",
    "        # Move data to device\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        outputs, _ = model(inputs)\n",
    "        outputs = outputs.reshape(-1, outputs.size(-1))\n",
    "        targets = targets.reshape(-1)\n",
    "\n",
    "        loss = criterion(outputs, targets)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        # Calculate accuracy\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += targets.size(0)\n",
    "        correct += (predicted == targets).sum().item()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Update progress bar\n",
    "        progress_bar.set_postfix({\n",
    "            'loss': f'{loss.item():.4f}',\n",
    "            'acc': f'{100 * correct / total:.2f}%'\n",
    "        })\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = 100 * correct / total\n",
    "\n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee3de09",
   "metadata": {},
   "source": [
    "## Step 11: Validation Function\n",
    "\n",
    "The validation function evaluates the model without updating weights. This helps us monitor overfitting and select the best model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1138482f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    progress_bar = tqdm.tqdm(dataloader, desc=\"Validating\")\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in progress_bar:\n",
    "            # Move data to device\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            outputs, _ = model(inputs)\n",
    "            outputs = outputs.reshape(-1, outputs.size(-1))\n",
    "            targets = targets.reshape(-1)\n",
    "\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            # Calculate accuracy\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += targets.size(0)\n",
    "            correct += (predicted == targets).sum().item()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Update progress bar with current accuracy\n",
    "            progress_bar.set_postfix({\n",
    "                'loss': f'{loss.item():.4f}',\n",
    "                'acc': f'{100 * correct / total:.2f}%'\n",
    "            })\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = 100 * correct / total\n",
    "\n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fc21b7e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Device: cuda\n",
      "Number of epochs: 20\n",
      "\n",
      "  Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 7811/7811 [13:32<00:00,  9.61it/s, loss=1.3049, acc=55.89%]\n",
      "Validating: 100%|██████████| 468/468 [00:21<00:00, 22.11it/s, loss=1.3411, acc=57.23%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20]\n",
      "  Train Loss: 1.4460 | Train Acc: 55.89%\n",
      "  Val Loss: 1.4052 | Val Acc: 57.23%\n",
      "  ✓ Saved best model (Val Acc: 57.23%)\n",
      "\n",
      "  Epoch 2/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 7811/7811 [14:32<00:00,  8.95it/s, loss=1.2902, acc=59.98%]\n",
      "Validating: 100%|██████████| 468/468 [00:24<00:00, 18.87it/s, loss=1.3543, acc=57.33%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/20]\n",
      "  Train Loss: 1.2910 | Train Acc: 59.98%\n",
      "  Val Loss: 1.4116 | Val Acc: 57.33%\n",
      "  ✓ Saved best model (Val Acc: 57.33%)\n",
      "\n",
      "  Epoch 3/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 7811/7811 [13:39<00:00,  9.53it/s, loss=1.2606, acc=60.78%]\n",
      "Validating: 100%|██████████| 468/468 [00:19<00:00, 24.28it/s, loss=1.3860, acc=57.21%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/20]\n",
      "  Train Loss: 1.2612 | Train Acc: 60.78%\n",
      "  Val Loss: 1.4235 | Val Acc: 57.21%\n",
      "\n",
      "  Epoch 4/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 7811/7811 [13:29<00:00,  9.65it/s, loss=1.2576, acc=61.21%]\n",
      "Validating: 100%|██████████| 468/468 [00:21<00:00, 21.78it/s, loss=1.3404, acc=57.26%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/20]\n",
      "  Train Loss: 1.2456 | Train Acc: 61.21%\n",
      "  Val Loss: 1.4307 | Val Acc: 57.26%\n",
      "\n",
      "  Epoch 5/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 7811/7811 [13:06<00:00,  9.93it/s, loss=1.2114, acc=61.89%]\n",
      "Validating: 100%|██████████| 468/468 [00:23<00:00, 20.06it/s, loss=1.3586, acc=57.13%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/20]\n",
      "  Train Loss: 1.2210 | Train Acc: 61.89%\n",
      "  Val Loss: 1.4382 | Val Acc: 57.13%\n",
      "\n",
      "  Epoch 6/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 7811/7811 [12:33<00:00, 10.37it/s, loss=1.1973, acc=62.06%]\n",
      "Validating: 100%|██████████| 468/468 [00:18<00:00, 24.65it/s, loss=1.3580, acc=56.93%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/20]\n",
      "  Train Loss: 1.2146 | Train Acc: 62.06%\n",
      "  Val Loss: 1.4429 | Val Acc: 56.93%\n",
      "\n",
      "  Epoch 7/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 7811/7811 [12:36<00:00, 10.32it/s, loss=1.2136, acc=62.18%]\n",
      "Validating: 100%|██████████| 468/468 [00:17<00:00, 26.41it/s, loss=1.3527, acc=57.03%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/20]\n",
      "  Train Loss: 1.2104 | Train Acc: 62.18%\n",
      "  Val Loss: 1.4423 | Val Acc: 57.03%\n",
      "\n",
      "  Epoch 8/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 7811/7811 [12:42<00:00, 10.25it/s, loss=1.1997, acc=62.53%]\n",
      "Validating: 100%|██████████| 468/468 [00:19<00:00, 24.20it/s, loss=1.3601, acc=56.96%]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/20]\n",
      "  Train Loss: 1.1983 | Train Acc: 62.53%\n",
      "  Val Loss: 1.4474 | Val Acc: 56.96%\n",
      "\n",
      "  Epoch 9/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   2%|▏         | 161/7811 [00:17<13:51,  9.21it/s, loss=1.1908, acc=62.63%]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 17\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  Epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Train\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m train_loss, train_acc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Validate\u001b[39;00m\n\u001b[0;32m     22\u001b[0m val_loss, val_acc \u001b[38;5;241m=\u001b[39m validate(model, valid_loader, criterion, device)\n",
      "Cell \u001b[1;32mIn[27], line 22\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[1;34m(model, dataloader, criterion, optimizer, device)\u001b[0m\n\u001b[0;32m     20\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, targets)\n\u001b[0;32m     21\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 22\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), max_norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5.0\u001b[39m)\n\u001b[0;32m     25\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\Users\\DIPLAB\\.conda\\envs\\ul-env\\Lib\\site-packages\\torch\\_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    580\u001b[0m     )\n\u001b[1;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\DIPLAB\\.conda\\envs\\ul-env\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\DIPLAB\\.conda\\envs\\ul-env\\Lib\\site-packages\\torch\\autograd\\graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 20\n",
    "best_val_acc = 0.0\n",
    "\n",
    "# Track training history\n",
    "train_losses = []\n",
    "train_accs = []\n",
    "val_losses = []\n",
    "val_accs = []\n",
    "\n",
    "print(\"Starting training...\")\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"Number of epochs: {num_epochs}\\n\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"  Epoch {epoch + 1}/{num_epochs}\")\n",
    "    # Train\n",
    "    train_loss, train_acc = train_epoch(\n",
    "        model, train_loader, criterion, optimizer, device\n",
    "    )\n",
    "\n",
    "    # Validate\n",
    "    val_loss, val_acc = validate(model, valid_loader, criterion, device)\n",
    "\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    # Track history\n",
    "    train_losses.append(train_loss)\n",
    "    train_accs.append(train_acc)\n",
    "    val_losses.append(val_loss)\n",
    "    val_accs.append(val_acc)\n",
    "\n",
    "    # Print progress\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}]\")\n",
    "    print(f\"  Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}%\")\n",
    "    print(f\"  Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}%\")\n",
    "\n",
    "    # Save best model\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(model.state_dict(), \"best_char_rnn_model.pth\")\n",
    "        print(f\"  ✓ Saved best model (Val Acc: {val_acc:.2f}%)\")\n",
    "    print()\n",
    "\n",
    "print(\"Training complete!\")\n",
    "print(f\"Best validation accuracy: {best_val_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d08b6f7",
   "metadata": {},
   "source": [
    "## Load Best Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "98ff78b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model loaded!\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(\n",
    "    torch.load(\"best_char_rnn_model.pth\", weights_only=True, map_location=device)\n",
    ")\n",
    "print(\"Best model loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22739e6f",
   "metadata": {},
   "source": [
    "## Next Character Prediction\n",
    "We can predict the next character given a seed text. This involves feeding the seed text into the model and sampling from the output probabilities to get the next character.\n",
    "\n",
    "### Temperature Sampling\n",
    "Instead of always picking the most likely character, we can use **temperature sampling** to add diversity:\n",
    "\n",
    "- **Temperature = 1**: Use probabilities as-is\n",
    "- **Temperature < 1**: Favor high-probability characters (more conservative)\n",
    "- **Temperature > 1**: Flatten probabilities (more creative/random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f13c1eb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: 'to be or not to b'\n",
      "Predicted next character: 'e'\n",
      "Result: 'to be or not to be'\n"
     ]
    }
   ],
   "source": [
    "def predict_next_char(\n",
    "    model, text, char_to_idx, idx_to_char, temperature=1.0, device=\"cpu\"\n",
    "):\n",
    "    model.eval()\n",
    "\n",
    "    text_lower = text.lower()\n",
    "    input_tensor = (\n",
    "        torch.tensor(\n",
    "            [char_to_idx.get(char, 0) for char in text_lower], dtype=torch.long\n",
    "        )\n",
    "        .unsqueeze(0)\n",
    "        .to(device)\n",
    "    )\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output, _ = model(input_tensor)\n",
    "\n",
    "        # Get logits for the last character\n",
    "        logits = output[0, -1, :]\n",
    "        scaled_logits = logits / temperature\n",
    "        probs = torch.softmax(scaled_logits, dim=0)\n",
    "        char_idx = torch.multinomial(probs, num_samples=1).item()\n",
    "\n",
    "    return idx_to_char[char_idx]\n",
    "\n",
    "\n",
    "# Test the function\n",
    "test_text = \"to be or not to b\"\n",
    "next_char = predict_next_char(\n",
    "    model, test_text, char_to_idx, idx_to_char, temperature=1.0, device=device\n",
    ")\n",
    "print(f\"Input: '{test_text}'\")\n",
    "print(f\"Predicted next character: '{next_char}'\")\n",
    "print(f\"Result: '{test_text + next_char}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736359a2",
   "metadata": {},
   "source": [
    "## Text Generation\n",
    "After training the model, we can use it to generate new text sequences.\n",
    "1. First, we give our model a seed text.\n",
    "2. Then, we predict the next character and append it to the sequence.\n",
    "3. Repeat the process for a desired length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e2581508",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Text Generation with Different Temperatures\n",
      "======================================================================\n",
      "\n",
      "Seed text: 'to be or not to be'\n",
      "\n",
      "======================================================================\n",
      "Temperature = 0.01 (Very Conservative)\n",
      "======================================================================\n",
      "to be or not to be a proportion\n",
      "and see the strength of the provost in the state,\n",
      "which we have seen the strong and se\n",
      "\n",
      "======================================================================\n",
      "Temperature = 1.0 (Balanced)\n",
      "======================================================================\n",
      "to be or not to be\n",
      "maided off, seening, sigrioal purses\n",
      "in them, this proud man that bear him to his\n",
      "with day will wea\n",
      "\n",
      "======================================================================\n",
      "Temperature = 2.0 (More Creative)\n",
      "======================================================================\n",
      "to be or not to be spice yet,\n",
      "isesadiey, and thy eandve thts'em aclondoble;\n",
      "wanioughfou sword to-fapet. let her prese\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def generate_text(\n",
    "    model,\n",
    "    seed_text,\n",
    "    char_to_idx,\n",
    "    idx_to_char,\n",
    "    num_chars=100,\n",
    "    temperature=1.0,\n",
    "    device=\"cuda\",\n",
    "):\n",
    "    generated_text = seed_text\n",
    "\n",
    "    # Generate characters one at a time\n",
    "    for _ in range(num_chars):\n",
    "        # Predict next character\n",
    "        next_char = predict_next_char(\n",
    "            model,\n",
    "            generated_text,\n",
    "            char_to_idx,\n",
    "            idx_to_char,\n",
    "            temperature=temperature,\n",
    "            device=device,\n",
    "        )\n",
    "        generated_text += next_char\n",
    "\n",
    "    return generated_text\n",
    "\n",
    "\n",
    "# Generate text with different temperatures\n",
    "seed = \"to be or not to be\"\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"Text Generation with Different Temperatures\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\nSeed text: '{seed}'\")\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"Temperature = 0.01 (Very Conservative)\")\n",
    "print(f\"{'='*70}\")\n",
    "text_low = generate_text(\n",
    "    model,\n",
    "    seed,\n",
    "    char_to_idx,\n",
    "    idx_to_char,\n",
    "    num_chars=100,\n",
    "    temperature=0.01,\n",
    "    device=device,\n",
    ")\n",
    "print(text_low)\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"Temperature = 1.0 (Balanced)\")\n",
    "print(f\"{'='*70}\")\n",
    "text_med = generate_text(\n",
    "    model, seed, char_to_idx, idx_to_char, num_chars=100, temperature=1.0, device=device\n",
    ")\n",
    "print(text_med)\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"Temperature = 2.0 (More Creative)\")\n",
    "print(f\"{'='*70}\")\n",
    "text_high = generate_text(\n",
    "    model, seed, char_to_idx, idx_to_char, num_chars=100, temperature=2.0, device=device\n",
    ")\n",
    "print(text_high)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b09c48",
   "metadata": {},
   "source": [
    "## Testing the Text Generation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d4773f1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom Text Generation\n",
      "======================================================================\n",
      "\n",
      "Seed: 'romeo and juliet'\n",
      "Generated: romeo and juliet,\n",
      "or for this peers perfection for his sport:\n",
      "your highness--as follows me.\n",
      "\n",
      "eithoraa:\n",
      "i pray thee, teech,\n",
      "go mourn perform'd and loves these towers.\n",
      "\n",
      "first murderer:\n",
      "what is cared my lord, is out in \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Seed: 'once upon a time'\n",
      "Generated: once upon a time,\n",
      "on there i do cries to your crown thus yours.\n",
      "\n",
      "gloucester:\n",
      "as followers to tite, and repoant my gitter;\n",
      "and most upench, but for my head spare here,\n",
      "the tribunes, throw of my benefit to-morrow.\n",
      "now \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Seed: 'the king said'\n",
      "Generated: the king said,\n",
      "on there is pecricious touchery of my point:\n",
      "applain the onay follow'd cloiding soul.\n",
      "\n",
      "prance:\n",
      "well eas, kites, nor it keeped to mistrust.\n",
      "\n",
      "coriolanus:\n",
      "move the king that he shall fetter them\n",
      "enough\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Seed: 'in fair verona'\n",
      "Generated: in fair verona;\n",
      "o, there is power--for the burn of my point:\n",
      "applain the onay follow'd cloiding soul.\n",
      "\n",
      "prance:\n",
      "well eas, kites, nor it keeped to mistrust.\n",
      "\n",
      "coriolanus:\n",
      "move the king that he shall fetter them\n",
      "enough\n",
      "----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Try different seed texts\n",
    "custom_seeds = [\n",
    "    \"romeo and juliet\",\n",
    "    \"once upon a time\",\n",
    "    \"the king said\",\n",
    "    \"in fair verona\",\n",
    "]\n",
    "\n",
    "print(\"Custom Text Generation\\n\" + \"=\" * 70)\n",
    "\n",
    "for seed in custom_seeds:\n",
    "    torch.manual_seed(42)  # Reset seed for consistency\n",
    "    generated = generate_text(\n",
    "        model,\n",
    "        seed,\n",
    "        char_to_idx,\n",
    "        idx_to_char,\n",
    "        num_chars=200,\n",
    "        temperature=1.0,\n",
    "        device=device,\n",
    "    )\n",
    "    print(f\"\\nSeed: '{seed}'\")\n",
    "    print(f\"Generated: {generated}\")\n",
    "    print(\"-\" * 70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ul-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
